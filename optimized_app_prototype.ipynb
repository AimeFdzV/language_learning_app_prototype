{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Learning App Prototype Notebook\n",
        "\n",
        "Welcome to the Language Learning App Prototype! This notebook demonstrates an initial prototype for a language learning application. The app predicts and translates words that a learner might find challenging based on their proficiency level and the words they mark as unknown.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Run All Cells**: To get started, you need to run all the cells in this notebook. This will install the necessary libraries, download required data, and set up the environment.\n",
        "\n",
        "2. **Interface Deployment**: Once all cells are run, a Gradio interface will be deployed at the bottom of this notebook. You will interact with this interface to use the app.\n",
        "\n",
        "3. **Using the Interface**:\n",
        "   - **Native Language**: Select your native language (currently only English is available).\n",
        "   - **Target Language**: Select the target language you want to learn (currently only Spanish is available).\n",
        "   - **Proficiency Level**: Choose your proficiency level from A1 to C2.\n",
        "   - **Text Input**: Enter the text you want to read and learn from in the target language.\n",
        "   - **Start**: Click the 'Start' button to begin the process. The app will process the text, predict unknown words, and provide translations.\n",
        "   - **Input Unknown Words**: You can input any additional unknown words you encounter.\n",
        "   - **Next Paragraph**: Click the 'Next Paragraph' button to process the next paragraph of text.\n",
        "   - **Restart**: If you want to start over, click the 'Restart' button to reset the interface.\n"
      ],
      "metadata": {
        "id": "9eoNE5YgLc3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o_qyrBCKgKR",
        "outputId": "34ea4963-dee0-425e-c5d6-f9efbb7a2fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
            "Successfully installed emoji-2.12.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 stanza-1.8.2\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=5ceb4793ebd172f5098e8f7c202997b51fef290f4898094764678fc7718d3eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.4.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.0.8)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (2024.5.15)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes>=3.0->wordfreq) (1.2.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from marisa-trie>=0.7.7->language-data>=1.2->langcodes>=3.0->wordfreq) (67.7.2)\n",
            "Installing collected packages: locate, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.2.0 locate-1.1.1 wordfreq-3.1.1\n",
            "Collecting wiktionaryparser\n",
            "  Downloading wiktionaryparser-0.0.97-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wiktionaryparser) (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wiktionaryparser) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wiktionaryparser) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wiktionaryparser) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wiktionaryparser) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wiktionaryparser) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wiktionaryparser) (2024.2.2)\n",
            "Installing collected packages: wiktionaryparser\n",
            "Successfully installed wiktionaryparser-0.0.97\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.32.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.17.0 (from gradio)\n",
            "  Downloading gradio_client-0.17.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.17.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.17.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=bd6b6dba1296369b18471396a422d459bbb1e051a78f68d039973421da731ba1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.32.2 gradio-client-0.17.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.7 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.9.3\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install stanza\n",
        "!pip install -U deep-translator\n",
        "!pip install langdetect\n",
        "!pip install wordfreq\n",
        "!pip install wiktionaryparser\n",
        "!pip install nltk\n",
        "!pip install gradio\n",
        "!pip install rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NEN-SF3DvqM",
        "outputId": "4cb6728c-b1c6-4efc-8a97-ad778c9184c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-02 16:33:25--  https://github.com/kbatsuren/CogNet/raw/master/CogNet-v2.0.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kbatsuren/CogNet/master/CogNet-v2.0.zip [following]\n",
            "--2024-06-02 16:33:25--  https://raw.githubusercontent.com/kbatsuren/CogNet/master/CogNet-v2.0.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47162529 (45M) [application/zip]\n",
            "Saving to: ‘CogNet-v2.0.zip’\n",
            "\n",
            "CogNet-v2.0.zip     100%[===================>]  44.98M   229MB/s    in 0.2s    \n",
            "\n",
            "2024-06-02 16:33:28 (229 MB/s) - ‘CogNet-v2.0.zip’ saved [47162529/47162529]\n",
            "\n",
            "Archive:  CogNet-v2.0.zip\n",
            "  inflating: CogNet-v2.0.tsv         \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._CogNet-v2.0.tsv  \n"
          ]
        }
      ],
      "source": [
        "# Download and extract the CogNet data\n",
        "!wget https://github.com/kbatsuren/CogNet/raw/master/CogNet-v2.0.zip\n",
        "!unzip CogNet-v2.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "import io\n",
        "import gradio as gr\n",
        "import stanza\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordfreq import word_frequency\n",
        "from rapidfuzz import fuzz, process\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Initialize Stanza pipelines (only once) with specific components\n",
        "stanza.download('en')\n",
        "stanza.download('es')\n",
        "nlp_native = stanza.Pipeline('en', processors='tokenize,lemma,pos')\n",
        "nlp_target = stanza.Pipeline('es', processors='tokenize,lemma,pos,ner')\n",
        "\n",
        "# Initialize the Snowball Stemmer\n",
        "stemmer = SnowballStemmer(\"spanish\")\n",
        "\n",
        "\n",
        "# Load the TSV file into a DataFrame, skipping bad lines and using the Python engine\n",
        "cognet_df = pd.read_csv('CogNet-v2.0.tsv', sep='\\t', header=None,\n",
        "                        names=['concept_id', 'lang1', 'word1', 'lang2', 'word2', 'translit1', 'translit2'],\n",
        "                        on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Filter for Spanish-English cognates\n",
        "cognet_sp_en = cognet_df[((cognet_df['lang1'] == 'spa') & (cognet_df['lang2'] == 'eng')) |\n",
        "                         ((cognet_df['lang1'] == 'eng') & (cognet_df['lang2'] == 'spa'))]\n"
      ],
      "metadata": {
        "id": "qHRi4gvdBTEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Variables to hold state\n",
        "state = {\n",
        "    'paragraphs': [],\n",
        "    'current_paragraph_index': 0,\n",
        "    'known_words': [],\n",
        "    'unknown_words': [],\n",
        "    'validated_translations': [],\n",
        "    'word_count': defaultdict(int),\n",
        "    'all_final_unknown_words': [],\n",
        "    'all_cognate_pairs': {},\n",
        "    'final_unknown_words_dict': defaultdict(set),\n",
        "    'original_word_mapping': {},\n",
        "    'native_language': '',\n",
        "    'target_language': '',\n",
        "    'level': '',\n",
        "    'final_unknown_word_counts': defaultdict(int),\n",
        "    'nlp_cache': {},\n",
        "    'frequency_cache': {},\n",
        "    'ner_cache': {}\n",
        "}\n",
        "\n",
        "frequency_thresholds = {\n",
        "    'A1': 0.0001,\n",
        "    'A2': 0.00001,\n",
        "    'B1': 0.000001,\n",
        "    'B2': 0.0000005,\n",
        "    'C1': 0.0000001,\n",
        "    'C2': 0.00000005\n",
        "}\n",
        "\n",
        "translation_cache = {}\n",
        "\n",
        "def initialize_variables():\n",
        "    global state\n",
        "    state = {\n",
        "        'paragraphs': [],\n",
        "        'current_paragraph_index': 0,\n",
        "        'known_words': [],\n",
        "        'unknown_words': [],\n",
        "        'validated_translations': [],\n",
        "        'word_count': defaultdict(int),\n",
        "        'all_final_unknown_words': [],\n",
        "        'all_cognate_pairs': {},\n",
        "        'final_unknown_words_dict': defaultdict(set),\n",
        "        'original_word_mapping': {},\n",
        "        'native_language': '',\n",
        "        'target_language': '',\n",
        "        'level': '',\n",
        "        'final_unknown_word_counts': defaultdict(int),\n",
        "        'nlp_cache': {},\n",
        "        'frequency_cache': {},\n",
        "        'ner_cache': {},\n",
        "        'merged_paragraphs': []\n",
        "    }\n",
        "\n",
        "# Function to identify cognates\n",
        "def find_cognates(spanish_words, english_words, cognet_df, similarity_threshold=0.6):\n",
        "    cognates = []\n",
        "    spanish_words_lower = [sp_word.lower() for sp_word in spanish_words]\n",
        "    english_words_lower = [en_word.lower() for en_word in english_words]\n",
        "\n",
        "    # Check for cognates in CogNet\n",
        "    for sp_word in spanish_words_lower:\n",
        "        matches = cognet_df[(cognet_df['word1'].str.lower() == sp_word) | (cognet_df['word2'].str.lower() == sp_word)]\n",
        "        for index, row in matches.iterrows():\n",
        "            if row['lang1'] == 'spa' and row['lang2'] == 'eng' and row['word2'].lower() in english_words_lower:\n",
        "                en_word = row['word2']\n",
        "            elif row['lang1'] == 'eng' and row['lang2'] == 'spa' and row['word1'].lower() in english_words_lower:\n",
        "                en_word = row['word1']\n",
        "            else:\n",
        "                continue\n",
        "            similarity = fuzz.ratio(sp_word, en_word.lower())\n",
        "            if similarity >= similarity_threshold * 100:\n",
        "                cognates.append((sp_word, en_word))\n",
        "\n",
        "    # Check for lemma-based cognates\n",
        "    for sp_word in spanish_words:\n",
        "        sp_features = state['nlp_cache'].get(sp_word, {})\n",
        "        for en_word in english_words:\n",
        "            en_features = state['nlp_cache'].get(en_word, {})\n",
        "            if sp_features and en_features and sp_features['lemma'] == en_features['lemma']:\n",
        "                cognates.append((sp_word, en_word))\n",
        "\n",
        "    return cognates\n",
        "\n",
        "# Safely translate a sentence with retry logic\n",
        "def safe_translate(sentences, src, dest, retries=3):\n",
        "    translations = []\n",
        "    for sentence in sentences:\n",
        "        if sentence in translation_cache:\n",
        "            translations.append(translation_cache[sentence])\n",
        "        else:\n",
        "            for _ in range(retries):\n",
        "                try:\n",
        "                    translation = GoogleTranslator(source=src, target=dest).translate(sentence)\n",
        "                    if translation:\n",
        "                        translations.append(translation)\n",
        "                        translation_cache[sentence] = translation\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error translating sentence '{sentence}': {e}\")\n",
        "                    time.sleep(1)\n",
        "            else:\n",
        "                translations.append(\"Translation not available\")\n",
        "    return translations\n",
        "\n",
        "# Batch translation\n",
        "def batch_translate(sentences, src, dest):\n",
        "    translations = []\n",
        "    for sentence in sentences:\n",
        "        if sentence in translation_cache:\n",
        "            translations.append(translation_cache[sentence])\n",
        "        else:\n",
        "            try:\n",
        "                translation = GoogleTranslator(source=src, target=dest).translate(sentence)\n",
        "                if translation:\n",
        "                    translations.append(translation)\n",
        "                    translation_cache[sentence] = translation\n",
        "                else:\n",
        "                    translations.append(\"Translation not available\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error translating sentence '{sentence}': {e}\")\n",
        "                translations.append(\"Translation not available\")\n",
        "    return translations\n",
        "\n",
        "# Function to check morphological similarity\n",
        "def is_similar_morphology(word1, word2, threshold):\n",
        "    if word1['stem'] == word2['stem']:\n",
        "        return True\n",
        "    if (word1['stem'] in word2['stem'] or word2['stem'] in word1['stem']) and word2['frequency'] < threshold:\n",
        "        return True\n",
        "    if word1['lemma'] == word2['lemma']:\n",
        "        return True\n",
        "    if (word1['lemma'] in word2['lemma'] or word2['lemma'] in word1['lemma']) and word2['frequency'] < threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Preprocess text (tokenize, lemmatize, POS tagging)\n",
        "def batch_preprocess_text(paragraphs, nlp, target_language, use_cache=True):\n",
        "    batch_text = \"\\n\\n\".join(paragraphs)\n",
        "    if use_cache and batch_text in state['nlp_cache']:\n",
        "        return state['nlp_cache'][batch_text]\n",
        "\n",
        "    doc = nlp(batch_text)\n",
        "    sentences = [sentence.text for sentence in doc.sentences]\n",
        "    words = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if len(word.text) > 1:  # Ensure we are processing only whole words\n",
        "                if word.text in state['frequency_cache']:\n",
        "                    frequency = state['frequency_cache'][word.text]\n",
        "                else:\n",
        "                    frequency = word_frequency(word.text, target_language)\n",
        "                    state['frequency_cache'][word.text] = frequency\n",
        "                word_features = {\n",
        "                    'text': word.text,\n",
        "                    'lemma': word.lemma,\n",
        "                    'pos': word.upos,\n",
        "                    'frequency': frequency,\n",
        "                    'stem': stemmer.stem(word.text)\n",
        "                }\n",
        "                words.append(word_features)\n",
        "                state['nlp_cache'][word.text.lower()] = word_features\n",
        "                print(f\"Added to cache: {word_features}\")  # Logging cache addition\n",
        "\n",
        "    if use_cache:\n",
        "        state['nlp_cache'][batch_text] = (sentences, words)\n",
        "    return sentences, words\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "def perform_ner(text, nlp):\n",
        "    if text in state['ner_cache']:\n",
        "        return state['ner_cache'][text]\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = [entity.text.lower() for sentence in doc.sentences for entity in sentence.ents]\n",
        "    state['ner_cache'][text] = entities\n",
        "    return entities\n",
        "\n",
        "# Validate translation with context\n",
        "def validate_translation_in_context(translation, original_sentences, translated_sentences, spanish_pos):\n",
        "    for orig_sent, trans_sent in zip(original_sentences, translated_sentences):\n",
        "        if trans_sent in state['nlp_cache']:\n",
        "            doc = state['nlp_cache'][trans_sent]\n",
        "        else:\n",
        "            doc = nlp_native(trans_sent)\n",
        "            state['nlp_cache'][trans_sent] = doc\n",
        "\n",
        "        if isinstance(doc, tuple):\n",
        "            doc = doc[1]  # Access the 'words' part of the tuple if it's a tuple\n",
        "\n",
        "        if orig_sent in state['nlp_cache']:\n",
        "            orig_doc = state['nlp_cache'][orig_sent]\n",
        "        else:\n",
        "            orig_doc = nlp_target(orig_sent)\n",
        "            state['nlp_cache'][orig_sent] = orig_doc\n",
        "\n",
        "        if isinstance(orig_doc, tuple):\n",
        "            orig_doc = orig_doc[1]  # Access the 'words' part of the tuple if it's a tuple\n",
        "\n",
        "        # Initial Check: Direct match\n",
        "        for word in doc.sentences[0].words:\n",
        "            if word.text.lower() == translation.lower():\n",
        "                return word.text\n",
        "\n",
        "        # Similarity Check: Find the most similar word\n",
        "        words_in_trans_sent = [word.text for word in doc.sentences[0].words]\n",
        "        most_similar = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=80)\n",
        "        similar_enough = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=70)\n",
        "        if most_similar:\n",
        "            similar_word = most_similar[0]\n",
        "            for word in doc.sentences[0].words:\n",
        "                if word.text == similar_word:\n",
        "                    return f\"{translation}/{similar_word}\"  # Return both the individual translation and the most similar word with the same POS tag\n",
        "        if similar_enough:\n",
        "            similar_enough_word = similar_enough[0]\n",
        "            for word in doc.sentences[0].words:\n",
        "                if word.text == similar_enough_word and word.upos == spanish_pos:\n",
        "                    return f\"{translation}/{similar_enough_word}\"\n",
        "        # POS Tag Check: Find a word with the same POS tag as the Spanish word\n",
        "        pos_matches = [word.text for word in doc.sentences[0].words if word.upos == spanish_pos]\n",
        "        if len(pos_matches) == 1:\n",
        "            return f\"{translation}/{pos_matches[0]}\"  # Return both the individual translation and the POS matching word if there's only one match\n",
        "        elif len(pos_matches) > 1:\n",
        "            return translation  # Stick to the individual translation if multiple POS matches are found\n",
        "\n",
        "    # If no word with the same POS tag is found\n",
        "    return translation\n",
        "\n",
        "# Profile Decorator\n",
        "def profile_func(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        pr = cProfile.Profile()\n",
        "        pr.enable()\n",
        "        result = func(*args, **kwargs)\n",
        "        pr.disable()\n",
        "        s = io.StringIO()\n",
        "        sortby = 'cumulative'\n",
        "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
        "        ps.print_stats(10)\n",
        "        print(s.getvalue())\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Translate and process each paragraph\n",
        "@profile_func\n",
        "def process_paragraph(paragraphs, input_unknown_words, known_words, unknown_words, validated_translations, word_count):\n",
        "    if not paragraphs:\n",
        "        return [], [], [], [], {}\n",
        "\n",
        "    current_paragraph_unknown_words = defaultdict(set)\n",
        "    sentences, words = batch_preprocess_text(paragraphs, nlp_target, state['target_language'])\n",
        "    entities = perform_ner(\"\\n\\n\".join(paragraphs), nlp_target)\n",
        "    translated_sentences = batch_translate(sentences, state['target_language'], state['native_language'])\n",
        "    threshold = frequency_thresholds[state['level']]\n",
        "\n",
        "    spanish_words = [word['text'].lower() for word in words]\n",
        "    english_words = batch_translate([word['text'] for word in words], state['target_language'], state['native_language'])\n",
        "    cognates = find_cognates(spanish_words, english_words, cognet_sp_en)\n",
        "    cognate_pairs = {sp: en for sp, en in cognates}\n",
        "\n",
        "    for word in words:\n",
        "        word_text = word['text'].lower()\n",
        "        state['original_word_mapping'][word_text] = word['text']\n",
        "        if word_text in entities or word['pos'] == 'PUNCT':\n",
        "            known_words.append(word)\n",
        "        elif word['frequency'] >= threshold or word_text in cognate_pairs:\n",
        "            known_words.append(word)\n",
        "        else:\n",
        "            state['final_unknown_words_dict'][word_text].add(word['lemma'])\n",
        "            current_paragraph_unknown_words[word_text].add(word['lemma'])\n",
        "\n",
        "    for word in words:\n",
        "        word_text = word['text'].lower()\n",
        "        if word_text in current_paragraph_unknown_words:\n",
        "            state['final_unknown_word_counts'][word_text] += 1\n",
        "            word_count[word_text] += 1\n",
        "\n",
        "            if state['final_unknown_word_counts'][word_text] >= 8:\n",
        "                del state['final_unknown_words_dict'][word_text]\n",
        "\n",
        "    input_unknown_word_details = []\n",
        "    for unknown_word in input_unknown_words:\n",
        "        if unknown_word:\n",
        "            processed_words = batch_preprocess_text([unknown_word], nlp_target, state['target_language'], use_cache=False)[1]\n",
        "            if processed_words:\n",
        "                processed_word = processed_words[0]\n",
        "                word_text = processed_word['text'].lower()\n",
        "                input_unknown_word_details.append(processed_word)\n",
        "                if word_text in spanish_words:\n",
        "                    state['final_unknown_words_dict'][word_text].add(processed_word['lemma'])\n",
        "                    current_paragraph_unknown_words[word_text].add(processed_word['lemma'])\n",
        "\n",
        "    for word in words:\n",
        "        for unknown_word in input_unknown_word_details:\n",
        "            if is_similar_morphology(word, unknown_word, threshold):\n",
        "                state['final_unknown_words_dict'][word['text'].lower()].add(word['lemma'])\n",
        "                current_paragraph_unknown_words[word['text'].lower()].add(word['lemma'])\n",
        "\n",
        "    for word_text, lemmas in state['final_unknown_words_dict'].items():\n",
        "        for word in words:\n",
        "            if is_similar_morphology({'text': word_text, 'lemma': next(iter(lemmas)), 'stem': stemmer.stem(word_text)}, word, threshold):\n",
        "                current_paragraph_unknown_words[word['text'].lower()].add(word['lemma'])\n",
        "\n",
        "    final_unknown_words = []\n",
        "    for word_text, lemmas in current_paragraph_unknown_words.items():\n",
        "        final_unknown_words.append({\n",
        "            'text': word_text,\n",
        "            'lemma': next(iter(lemmas)),\n",
        "            'pos': next((word['pos'] for word in words if word['text'].lower() == word_text), 'UNKNOWN'),\n",
        "            'frequency': next((word['frequency'] for word in words if word['text'].lower() == word_text), 0.0),\n",
        "            'stem': stemmer.stem(word_text)\n",
        "        })\n",
        "\n",
        "    def map_words_to_sentences(sentences, words):\n",
        "        sentence_word_map = {}\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            for word in words:\n",
        "                if word['text'].lower() in sentence.lower():\n",
        "                    sentence_word_map[word['text'].lower()] = (sentence, i)\n",
        "        return sentence_word_map\n",
        "\n",
        "    sentence_word_map = map_words_to_sentences(sentences, final_unknown_words)\n",
        "    for word in final_unknown_words:\n",
        "        if word['text'].lower() not in sentence_word_map:\n",
        "            continue\n",
        "        if word['text'] in translation_cache:\n",
        "            translation = translation_cache[word['text']]\n",
        "        else:\n",
        "            translation = GoogleTranslator(source=state['target_language'], target=state['native_language']).translate(word['text'])\n",
        "            translation_cache[word['text']] = translation\n",
        "        validated_translation = validate_translation_in_context(\n",
        "            translation,\n",
        "            sentences,\n",
        "            translated_sentences,\n",
        "            word['pos']\n",
        "        )\n",
        "        word['translation'] = validated_translation\n",
        "        word['sentence'] = sentence_word_map[word['text'].lower()][0]\n",
        "        word['translated_sentence'] = translated_sentences[sentence_word_map[word['text'].lower()][1]]\n",
        "        validated_translations.append({\n",
        "            'original': word['text'],\n",
        "            'translation': validated_translation,\n",
        "            'translated_pos': word['pos']\n",
        "        })\n",
        "\n",
        "    return sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs\n",
        "\n",
        "@profile_func\n",
        "def start_processing(native_language, target_language, level, text):\n",
        "    initialize_variables()\n",
        "    state['native_language'] = native_language\n",
        "    state['target_language'] = target_language\n",
        "    state['level'] = level\n",
        "\n",
        "    state['paragraphs'] = text.strip().split('\\n')\n",
        "    state['current_paragraph_index'] = 0\n",
        "\n",
        "    return process_next_paragraph([])\n",
        "\n",
        "def process_next_paragraph(input_unknown_words):\n",
        "    global state\n",
        "    if state['current_paragraph_index'] < len(state['paragraphs']):\n",
        "        paragraph = state['paragraphs'][state['current_paragraph_index']].strip()\n",
        "        if not paragraph:\n",
        "            state['current_paragraph_index'] += 1\n",
        "            return process_next_paragraph(input_unknown_words)\n",
        "        sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs = process_paragraph(\n",
        "            [paragraph],\n",
        "            input_unknown_words,\n",
        "            state['known_words'],\n",
        "            state['unknown_words'],\n",
        "            state['validated_translations'],\n",
        "            state['word_count']\n",
        "        )\n",
        "        state['all_final_unknown_words'].extend(final_unknown_words)\n",
        "        state['all_cognate_pairs'].update(cognate_pairs)\n",
        "        output = display_output(paragraph, final_unknown_words)\n",
        "        state['current_paragraph_index'] += 1\n",
        "        print(f\"Paragraph {state['current_paragraph_index']} processed with input unknown words: {input_unknown_words}\")  # Debugging: Print paragraph processing status\n",
        "        return output\n",
        "    else:\n",
        "        summary = generate_summary()\n",
        "        return f\"All paragraphs processed<br>{summary}\"\n",
        "\n",
        "def highlighted_paragraph(paragraph, final_unknown_words, validated_translations):\n",
        "    def preserve_case_replace(match, replacement):\n",
        "        matched_text = match.group()\n",
        "        if matched_text.isupper():\n",
        "            return replacement.upper()\n",
        "        elif matched_text[0].isupper():\n",
        "            return replacement.capitalize()\n",
        "        else:\n",
        "            return replacement\n",
        "\n",
        "    highlighted_paragraph = paragraph\n",
        "    for word in final_unknown_words:\n",
        "        original_word = state['original_word_mapping'].get(word['text'], word['text'])\n",
        "        translation_info = next((item for item in validated_translations if item['original'] == word['text']), None)\n",
        "        if translation_info:\n",
        "            translation = translation_info['translation']\n",
        "            highlighted_paragraph = re.sub(r'\\b{}\\b'.format(re.escape(original_word)),\n",
        "                                           lambda match: preserve_case_replace(match, f\"<b>{original_word}</b>({translation})\"),\n",
        "                                           highlighted_paragraph, flags=re.IGNORECASE)\n",
        "    return highlighted_paragraph\n",
        "\n",
        "def display_output(paragraph, final_unknown_words):\n",
        "    highlighted_para = highlighted_paragraph(paragraph, final_unknown_words, state['validated_translations'])\n",
        "    context_sentences = []\n",
        "    for word in final_unknown_words:\n",
        "        translation = word.get('translation', 'No translation available')\n",
        "        context_sentence = f\"<b>{word['text']}:</b> <b>{translation}</b>.<br>{word.get('translated_sentence', 'No sentence available')}<br>\"\n",
        "        context_sentences.append(context_sentence)\n",
        "\n",
        "    context_output = \"<br>\".join(context_sentences)\n",
        "    original_paragraphs = paragraph.split(' ')\n",
        "    highlighted_original_para = highlighted_paragraph(\" \".join(original_paragraphs), final_unknown_words, state['validated_translations'])\n",
        "\n",
        "    return f\"<p><b style='font-size: larger;'>Highlighted Text:</b></p><p>{highlighted_original_para}</p><hr><p><b style='font-size: larger;'>Predicted Unknown Words In Context:</b></p><p>{context_output}</p>\"\n",
        "\n",
        "def generate_summary():\n",
        "    summary = \"<p style='font-size: larger;'><b>Summary of Unknown Words:</b></p><br>\"\n",
        "\n",
        "    translations_dict = {word: next((item for item in state['validated_translations'] if item['original'] == word), {}).get('translation', 'No translation found')\n",
        "                         for word in state['final_unknown_word_counts'].keys()}\n",
        "\n",
        "    for word, count in state['final_unknown_word_counts'].items():\n",
        "        translation = translations_dict.get(word, 'No translation found')\n",
        "        summary += f\"<b>{word}:</b> {count} appearances, Translation: {translation}<br>\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "def next_paragraph(input_unknown_words):\n",
        "    if isinstance(input_unknown_words, str):\n",
        "        input_unknown_words = input_unknown_words.split()\n",
        "    return process_next_paragraph(input_unknown_words)\n",
        "\n",
        "def reset_interface():\n",
        "    initialize_variables()\n",
        "    return gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value='')\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Blocks()\n",
        "\n",
        "with iface:\n",
        "    native_language_input = gr.Dropdown(choices=['en'], label='Native Language', value='en')\n",
        "    target_language_input = gr.Dropdown(choices=['es'], label='Target Language')\n",
        "    level_input = gr.Dropdown(choices=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], label='Level')\n",
        "    text_input = gr.Textbox(label='Text', lines=10)\n",
        "    start_button = gr.Button('Start')\n",
        "    output_area = gr.HTML()\n",
        "    unknown_words_input = gr.Textbox(label='Input Unknown Words', lines=2)\n",
        "    next_button = gr.Button('Next Paragraph')\n",
        "    restart_button = gr.Button('Restart')\n",
        "\n",
        "    start_button.click(start_processing, [native_language_input, target_language_input, level_input, text_input], [output_area])\n",
        "    next_button.click(next_paragraph, [unknown_words_input], [output_area])\n",
        "    restart_button.click(reset_interface, [], [native_language_input, target_language_input, level_input, text_input, output_area, unknown_words_input])\n",
        "\n",
        "iface.launch(share=True, debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0GufNasUbOri",
        "outputId": "646b8433-8c7a-45de-ea19-ea30b4794165"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://451d9196a4d0681db4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://451d9196a4d0681db4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added to cache: {'text': 'actor', 'lemma': 'actor', 'pos': 'NOUN', 'frequency': 3.39e-05, 'stem': 'actor'}\n",
            "Added to cache: {'text': 'animal', 'lemma': 'animal', 'pos': 'NOUN', 'frequency': 4.68e-05, 'stem': 'animal'}\n",
            "Added to cache: {'text': 'hospital', 'lemma': 'hospital', 'pos': 'NOUN', 'frequency': 8.71e-05, 'stem': 'hospital'}\n",
            "Added to cache: {'text': 'color', 'lemma': 'color', 'pos': 'NOUN', 'frequency': 0.000115, 'stem': 'color'}\n",
            "Added to cache: {'text': 'doctor', 'lemma': 'doctor', 'pos': 'NOUN', 'frequency': 8.13e-05, 'stem': 'doctor'}\n",
            "Added to cache: {'text': 'family', 'lemma': 'family', 'pos': 'NOUN', 'frequency': 3.72e-06, 'stem': 'family'}\n",
            "Added to cache: {'text': 'idea', 'lemma': 'idea', 'pos': 'NOUN', 'frequency': 0.000269, 'stem': 'ide'}\n",
            "Added to cache: {'text': 'radio', 'lemma': 'radio', 'pos': 'NOUN', 'frequency': 0.000107, 'stem': 'radi'}\n",
            "Added to cache: {'text': 'natural', 'lemma': 'natural', 'pos': 'ADJ', 'frequency': 0.000107, 'stem': 'natural'}\n",
            "Added to cache: {'text': 'final', 'lemma': 'final', 'pos': 'ADJ', 'frequency': 0.000316, 'stem': 'final'}\n",
            "Added to cache: {'text': 'chocolate', 'lemma': 'chocolate', 'pos': 'NOUN', 'frequency': 2.57e-05, 'stem': 'chocolat'}\n",
            "Added to cache: {'text': 'banana', 'lemma': 'banana', 'pos': 'NOUN', 'frequency': 3.98e-06, 'stem': 'banan'}\n",
            "Added to cache: {'text': 'piano', 'lemma': 'piano', 'pos': 'NOUN', 'frequency': 1.48e-05, 'stem': 'pian'}\n",
            "Added to cache: {'text': 'general', 'lemma': 'general', 'pos': 'NOUN', 'frequency': 0.000437, 'stem': 'general'}\n",
            "Added to cache: {'text': 'material', 'lemma': 'material', 'pos': 'NOUN', 'frequency': 8.13e-05, 'stem': 'material'}\n",
            "Added to cache: {'text': 'particular', 'lemma': 'particular', 'pos': 'ADJ', 'frequency': 8.51e-05, 'stem': 'particul'}\n",
            "Added to cache: {'text': 'probable', 'lemma': 'probable', 'pos': 'ADJ', 'frequency': 4.57e-05, 'stem': 'probabl'}\n",
            "Added to cache: {'text': 'regular', 'lemma': 'regular', 'pos': 'ADJ', 'frequency': 3.24e-05, 'stem': 'regul'}\n",
            "Added to cache: {'text': 'similar', 'lemma': 'similar', 'pos': 'ADJ', 'frequency': 6.92e-05, 'stem': 'simil'}\n",
            "Added to cache: {'text': 'accident', 'lemma': 'accident', 'pos': 'NOUN', 'frequency': 1.29e-07, 'stem': 'accident'}\n",
            "         355225 function calls (350766 primitive calls) in 7.579 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 1409 to 10 due to restriction <10>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000    7.581    7.581 <ipython-input-4-564c21bdca87>:246(process_paragraph)\n",
            "        2    0.002    0.001    3.597    1.798 <ipython-input-4-564c21bdca87>:108(batch_translate)\n",
            "       21    0.001    0.000    3.549    0.169 /usr/local/lib/python3.10/dist-packages/deep_translator/google.py:51(translate)\n",
            "       21    0.001    0.000    3.440    0.164 /usr/local/lib/python3.10/dist-packages/requests/api.py:62(get)\n",
            "       21    0.000    0.000    3.440    0.164 /usr/local/lib/python3.10/dist-packages/requests/api.py:14(request)\n",
            "       21    0.000    0.000    3.434    0.164 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:502(request)\n",
            "       21    0.001    0.000    3.403    0.162 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:673(send)\n",
            "        3    0.000    0.000    2.445    0.815 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:479(__call__)\n",
            "        3    0.000    0.000    2.445    0.815 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:391(process)\n",
            "   740/94    0.002    0.000    2.369    0.025 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1528(_wrapped_call_impl)\n",
            "\n",
            "\n",
            "\n",
            "Paragraph 1 processed with input unknown words: []\n",
            "         9 function calls in 7.620 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000    7.620    7.620 <ipython-input-4-564c21bdca87>:349(start_processing)\n",
            "        1    0.000    0.000    7.620    7.620 <ipython-input-4-564c21bdca87>:361(process_next_paragraph)\n",
            "        1    0.000    0.000    7.620    7.620 <ipython-input-4-564c21bdca87>:232(wrapper)\n",
            "        1    7.620    7.620    7.620    7.620 {method 'enable' of '_lsprof.Profiler' objects}\n",
            "        1    0.000    0.000    0.000    0.000 <ipython-input-4-564c21bdca87>:33(initialize_variables)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/dropdown.py:181: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include:  or set allow_custom_value=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added to cache: {'text': 'La', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'Dificultad', 'lemma': 'dificultad', 'pos': 'NOUN', 'frequency': 1.86e-05, 'stem': 'dificult'}\n",
            "Added to cache: {'text': 'para', 'lemma': 'para', 'pos': 'ADP', 'frequency': 0.00832, 'stem': 'par'}\n",
            "Added to cache: {'text': 'Concentrar', 'lemma': 'concentrar', 'pos': 'VERB', 'frequency': 2.95e-06, 'stem': 'concentr'}\n",
            "Added to cache: {'text': 'se', 'lemma': 'él', 'pos': 'PRON', 'frequency': 0.0115, 'stem': 'se'}\n",
            "         122969 function calls (119454 primitive calls) in 1.924 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 1432 to 10 due to restriction <10>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000    1.924    1.924 <ipython-input-4-564c21bdca87>:246(process_paragraph)\n",
            "        7    0.000    0.000    1.027    0.147 /usr/local/lib/python3.10/dist-packages/deep_translator/google.py:51(translate)\n",
            "        7    0.000    0.000    0.999    0.143 /usr/local/lib/python3.10/dist-packages/requests/api.py:62(get)\n",
            "        7    0.000    0.000    0.999    0.143 /usr/local/lib/python3.10/dist-packages/requests/api.py:14(request)\n",
            "        7    0.000    0.000    0.997    0.142 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:502(request)\n",
            "        7    0.000    0.000    0.988    0.141 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:673(send)\n",
            "        2    0.001    0.000    0.907    0.453 <ipython-input-4-564c21bdca87>:108(batch_translate)\n",
            "        3    0.000    0.000    0.619    0.206 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:479(__call__)\n",
            "        3    0.000    0.000    0.619    0.206 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:391(process)\n",
            "        7    0.000    0.000    0.589    0.084 /usr/local/lib/python3.10/dist-packages/requests/adapters.py:434(send)\n",
            "\n",
            "\n",
            "\n",
            "Paragraph 1 processed with input unknown words: []\n",
            "         9 function calls in 1.944 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.000    0.000    1.944    1.944 <ipython-input-4-564c21bdca87>:349(start_processing)\n",
            "        1    0.000    0.000    1.944    1.944 <ipython-input-4-564c21bdca87>:361(process_next_paragraph)\n",
            "        1    0.000    0.000    1.944    1.944 <ipython-input-4-564c21bdca87>:232(wrapper)\n",
            "        1    1.944    1.944    1.944    1.944 {method 'enable' of '_lsprof.Profiler' objects}\n",
            "        1    0.000    0.000    0.000    0.000 <ipython-input-4-564c21bdca87>:33(initialize_variables)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
            "\n",
            "\n",
            "\n",
            "Added to cache: {'text': 'Uno', 'lemma': 'uno', 'pos': 'PRON', 'frequency': 0.000912, 'stem': 'uno'}\n",
            "Added to cache: {'text': 'de', 'lemma': 'de', 'pos': 'ADP', 'frequency': 0.0646, 'stem': 'de'}\n",
            "Added to cache: {'text': 'los', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0151, 'stem': 'los'}\n",
            "Added to cache: {'text': 'principales', 'lemma': 'principal', 'pos': 'ADJ', 'frequency': 0.00012, 'stem': 'principal'}\n",
            "Added to cache: {'text': 'desafíos', 'lemma': 'desafío', 'pos': 'NOUN', 'frequency': 9.12e-06, 'stem': 'desafi'}\n",
            "Added to cache: {'text': 'que', 'lemma': 'que', 'pos': 'PRON', 'frequency': 0.0331, 'stem': 'que'}\n",
            "Added to cache: {'text': 'enfrentan', 'lemma': 'enfrentar', 'pos': 'VERB', 'frequency': 8.13e-06, 'stem': 'enfrent'}\n",
            "Added to cache: {'text': 'los', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0151, 'stem': 'los'}\n",
            "Added to cache: {'text': 'niños', 'lemma': 'niño', 'pos': 'NOUN', 'frequency': 0.000245, 'stem': 'niñ'}\n",
            "Added to cache: {'text': 'en', 'lemma': 'en', 'pos': 'ADP', 'frequency': 0.0282, 'stem': 'en'}\n",
            "Added to cache: {'text': 'la', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'era', 'lemma': 'era', 'pos': 'NOUN', 'frequency': 0.00112, 'stem': 'era'}\n",
            "Added to cache: {'text': 'digital', 'lemma': 'digital', 'pos': 'ADJ', 'frequency': 5.75e-05, 'stem': 'digital'}\n",
            "Added to cache: {'text': 'es', 'lemma': 'ser', 'pos': 'AUX', 'frequency': 0.0105, 'stem': 'es'}\n",
            "Added to cache: {'text': 'la', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'dificultad', 'lemma': 'dificultad', 'pos': 'NOUN', 'frequency': 1.86e-05, 'stem': 'dificult'}\n",
            "Added to cache: {'text': 'para', 'lemma': 'para', 'pos': 'ADP', 'frequency': 0.00832, 'stem': 'par'}\n",
            "Added to cache: {'text': 'concentrar', 'lemma': 'concentrar', 'pos': 'VERB', 'frequency': 2.95e-06, 'stem': 'concentr'}\n",
            "Added to cache: {'text': 'se', 'lemma': 'él', 'pos': 'PRON', 'frequency': 0.0115, 'stem': 'se'}\n",
            "Added to cache: {'text': 'La', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'abundancia', 'lemma': 'abundancia', 'pos': 'NOUN', 'frequency': 8.32e-06, 'stem': 'abund'}\n",
            "Added to cache: {'text': 'de', 'lemma': 'de', 'pos': 'ADP', 'frequency': 0.0646, 'stem': 'de'}\n",
            "Added to cache: {'text': 'dispositivos', 'lemma': 'dispositivo', 'pos': 'NOUN', 'frequency': 1.58e-05, 'stem': 'disposit'}\n",
            "Added to cache: {'text': 'electrónicos', 'lemma': 'electrónico', 'pos': 'ADJ', 'frequency': 1.45e-05, 'stem': 'electron'}\n",
            "Added to cache: {'text': 'el', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0282, 'stem': 'el'}\n",
            "Added to cache: {'text': 'acceso', 'lemma': 'acceso', 'pos': 'NOUN', 'frequency': 0.000112, 'stem': 'acces'}\n",
            "Added to cache: {'text': 'constante', 'lemma': 'constante', 'pos': 'ADJ', 'frequency': 4.27e-05, 'stem': 'constant'}\n",
            "Added to cache: {'text': 'internet', 'lemma': 'internet', 'pos': 'NOUN', 'frequency': 0.000151, 'stem': 'internet'}\n",
            "Added to cache: {'text': 'han', 'lemma': 'haber', 'pos': 'AUX', 'frequency': 0.000851, 'stem': 'han'}\n",
            "Added to cache: {'text': 'cambiado', 'lemma': 'cambiar', 'pos': 'VERB', 'frequency': 4.07e-05, 'stem': 'cambi'}\n",
            "Added to cache: {'text': 'la', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'manera', 'lemma': 'manera', 'pos': 'NOUN', 'frequency': 0.000407, 'stem': 'maner'}\n",
            "Added to cache: {'text': 'el', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0282, 'stem': 'el'}\n",
            "Added to cache: {'text': 'modo', 'lemma': 'modo', 'pos': 'NOUN', 'frequency': 0.000155, 'stem': 'mod'}\n",
            "Added to cache: {'text': 'en', 'lemma': 'en', 'pos': 'ADP', 'frequency': 0.0282, 'stem': 'en'}\n",
            "Added to cache: {'text': 'que', 'lemma': 'que', 'pos': 'PRON', 'frequency': 0.0331, 'stem': 'que'}\n",
            "Added to cache: {'text': 'los', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0151, 'stem': 'los'}\n",
            "Added to cache: {'text': 'niños', 'lemma': 'niño', 'pos': 'NOUN', 'frequency': 0.000245, 'stem': 'niñ'}\n",
            "Added to cache: {'text': 'interactúan', 'lemma': 'interactuar', 'pos': 'VERB', 'frequency': 1.45e-06, 'stem': 'interactu'}\n",
            "Added to cache: {'text': 'con', 'lemma': 'con', 'pos': 'ADP', 'frequency': 0.00933, 'stem': 'con'}\n",
            "Added to cache: {'text': 'el', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0282, 'stem': 'el'}\n",
            "Added to cache: {'text': 'mundo', 'lemma': 'mundo', 'pos': 'NOUN', 'frequency': 0.000776, 'stem': 'mund'}\n",
            "Added to cache: {'text': 'procesan', 'lemma': 'procesar', 'pos': 'VERB', 'frequency': 1e-06, 'stem': 'proces'}\n",
            "Added to cache: {'text': 'la', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'información', 'lemma': 'información', 'pos': 'NOUN', 'frequency': 0.000302, 'stem': 'inform'}\n",
            "Added to cache: {'text': 'Las', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.00977, 'stem': 'las'}\n",
            "Added to cache: {'text': 'redes', 'lemma': 'red', 'pos': 'NOUN', 'frequency': 9.55e-05, 'stem': 'red'}\n",
            "Added to cache: {'text': 'sociales', 'lemma': 'social', 'pos': 'ADJ', 'frequency': 0.000186, 'stem': 'social'}\n",
            "Added to cache: {'text': 'los', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0151, 'stem': 'los'}\n",
            "Added to cache: {'text': 'videojuegos', 'lemma': 'videojuego', 'pos': 'NOUN', 'frequency': 1.7e-05, 'stem': 'videojueg'}\n",
            "Added to cache: {'text': 'las', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.00977, 'stem': 'las'}\n",
            "Added to cache: {'text': 'aplicaciones', 'lemma': 'aplicación', 'pos': 'NOUN', 'frequency': 3.09e-05, 'stem': 'aplic'}\n",
            "Added to cache: {'text': 'móviles', 'lemma': 'móvil', 'pos': 'ADJ', 'frequency': 2.14e-05, 'stem': 'movil'}\n",
            "Added to cache: {'text': 'ofrecen', 'lemma': 'ofrecer', 'pos': 'VERB', 'frequency': 2.57e-05, 'stem': 'ofrec'}\n",
            "Added to cache: {'text': 'estímulos', 'lemma': 'estímulo', 'pos': 'NOUN', 'frequency': 3.31e-06, 'stem': 'estimul'}\n",
            "Added to cache: {'text': 'constantes', 'lemma': 'constante', 'pos': 'ADJ', 'frequency': 1.2e-05, 'stem': 'constant'}\n",
            "Added to cache: {'text': 'que', 'lemma': 'que', 'pos': 'PRON', 'frequency': 0.0331, 'stem': 'que'}\n",
            "Added to cache: {'text': 'pueden', 'lemma': 'poder', 'pos': 'AUX', 'frequency': 0.000479, 'stem': 'pued'}\n",
            "Added to cache: {'text': 'resultar', 'lemma': 'resultar', 'pos': 'VERB', 'frequency': 1.2e-05, 'stem': 'result'}\n",
            "Added to cache: {'text': 'en', 'lemma': 'en', 'pos': 'ADP', 'frequency': 0.0282, 'stem': 'en'}\n",
            "Added to cache: {'text': 'una', 'lemma': 'uno', 'pos': 'DET', 'frequency': 0.00891, 'stem': 'una'}\n",
            "Added to cache: {'text': 'menor', 'lemma': 'menor', 'pos': 'ADJ', 'frequency': 0.000115, 'stem': 'menor'}\n",
            "Added to cache: {'text': 'capacidad', 'lemma': 'capacidad', 'pos': 'NOUN', 'frequency': 0.000107, 'stem': 'capac'}\n",
            "Added to cache: {'text': 'de', 'lemma': 'de', 'pos': 'ADP', 'frequency': 0.0646, 'stem': 'de'}\n",
            "Added to cache: {'text': 'atención', 'lemma': 'atención', 'pos': 'NOUN', 'frequency': 0.000182, 'stem': 'atencion'}\n",
            "Added to cache: {'text': 'un', 'lemma': 'uno', 'pos': 'DET', 'frequency': 0.0117, 'stem': 'un'}\n",
            "Added to cache: {'text': 'aumento', 'lemma': 'aumento', 'pos': 'NOUN', 'frequency': 7.76e-05, 'stem': 'aument'}\n",
            "Added to cache: {'text': 'en', 'lemma': 'en', 'pos': 'ADP', 'frequency': 0.0282, 'stem': 'en'}\n",
            "Added to cache: {'text': 'la', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0363, 'stem': 'la'}\n",
            "Added to cache: {'text': 'dispersión', 'lemma': 'dispersión', 'pos': 'NOUN', 'frequency': 4.27e-06, 'stem': 'dispersion'}\n",
            "Added to cache: {'text': 'Este', 'lemma': 'este', 'pos': 'DET', 'frequency': 0.00234, 'stem': 'este'}\n",
            "Added to cache: {'text': 'entorno', 'lemma': 'entorno', 'pos': 'NOUN', 'frequency': 3.24e-05, 'stem': 'entorn'}\n",
            "Added to cache: {'text': 'hiperconectado', 'lemma': 'hiperconectado', 'pos': 'ADJ', 'frequency': 4.27e-08, 'stem': 'hiperconect'}\n",
            "Added to cache: {'text': 'puede', 'lemma': 'poder', 'pos': 'AUX', 'frequency': 0.00129, 'stem': 'pued'}\n",
            "Added to cache: {'text': 'hacer', 'lemma': 'hacer', 'pos': 'VERB', 'frequency': 0.00117, 'stem': 'hac'}\n",
            "Added to cache: {'text': 'que', 'lemma': 'que', 'pos': 'SCONJ', 'frequency': 0.0331, 'stem': 'que'}\n",
            "Added to cache: {'text': 'los', 'lemma': 'el', 'pos': 'DET', 'frequency': 0.0151, 'stem': 'los'}\n",
            "Added to cache: {'text': 'métodos', 'lemma': 'método', 'pos': 'NOUN', 'frequency': 3.47e-05, 'stem': 'metod'}\n",
            "Added to cache: {'text': 'tradicionales', 'lemma': 'tradicional', 'pos': 'ADJ', 'frequency': 3.09e-05, 'stem': 'tradicional'}\n",
            "Added to cache: {'text': 'de', 'lemma': 'de', 'pos': 'ADP', 'frequency': 0.0646, 'stem': 'de'}\n",
            "Added to cache: {'text': 'enseñanza', 'lemma': 'enseñanza', 'pos': 'NOUN', 'frequency': 3.16e-05, 'stem': 'enseñ'}\n",
            "Added to cache: {'text': 'que', 'lemma': 'que', 'pos': 'PRON', 'frequency': 0.0331, 'stem': 'que'}\n",
            "Added to cache: {'text': 'requieren', 'lemma': 'requerir', 'pos': 'VERB', 'frequency': 1.48e-05, 'stem': 'requier'}\n",
            "Added to cache: {'text': 'largos', 'lemma': 'largo', 'pos': 'ADJ', 'frequency': 1.91e-05, 'stem': 'larg'}\n",
            "Added to cache: {'text': 'períodos', 'lemma': 'período', 'pos': 'NOUN', 'frequency': 8.51e-06, 'stem': 'period'}\n",
            "Added to cache: {'text': 'de', 'lemma': 'de', 'pos': 'ADP', 'frequency': 0.0646, 'stem': 'de'}\n",
            "Added to cache: {'text': 'atención', 'lemma': 'atención', 'pos': 'NOUN', 'frequency': 0.000182, 'stem': 'atencion'}\n",
            "Added to cache: {'text': 'sostenida', 'lemma': 'sostenido', 'pos': 'ADJ', 'frequency': 3.09e-06, 'stem': 'sosten'}\n",
            "Added to cache: {'text': 'resulten', 'lemma': 'resultar', 'pos': 'VERB', 'frequency': 2.4e-06, 'stem': 'result'}\n",
            "Added to cache: {'text': 'menos', 'lemma': 'menos', 'pos': 'ADV', 'frequency': 0.000692, 'stem': 'men'}\n",
            "Added to cache: {'text': 'efectivos', 'lemma': 'efectivo', 'pos': 'ADJ', 'frequency': 1.02e-05, 'stem': 'efect'}\n",
            "         1491949 function calls (1481363 primitive calls) in 25.137 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 1438 to 10 due to restriction <10>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.009    0.009   25.142   25.142 <ipython-input-4-564c21bdca87>:246(process_paragraph)\n",
            "        2    0.005    0.003   11.506    5.753 <ipython-input-4-564c21bdca87>:108(batch_translate)\n",
            "       68    0.003    0.000   11.370    0.167 /usr/local/lib/python3.10/dist-packages/deep_translator/google.py:51(translate)\n",
            "       68    0.002    0.000   11.075    0.163 /usr/local/lib/python3.10/dist-packages/requests/api.py:62(get)\n",
            "       68    0.001    0.000   11.074    0.163 /usr/local/lib/python3.10/dist-packages/requests/api.py:14(request)\n",
            "       68    0.002    0.000   11.058    0.163 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:502(request)\n",
            "       68    0.003    0.000   10.965    0.161 /usr/local/lib/python3.10/dist-packages/requests/sessions.py:673(send)\n",
            "        4    0.000    0.000    6.935    1.734 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:479(__call__)\n",
            "        4    0.001    0.000    6.935    1.734 /usr/local/lib/python3.10/dist-packages/stanza/pipeline/core.py:391(process)\n",
            " 1443/322    0.005    0.000    6.705    0.021 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1528(_wrapped_call_impl)\n",
            "\n",
            "\n",
            "\n",
            "Paragraph 2 processed with input unknown words: []\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://451d9196a4d0681db4.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
