{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eoNE5YgLc3l"
      },
      "source": [
        "# Language Learning App Prototype Notebook\n",
        "\n",
        "Welcome to the Language Learning App Prototype! This notebook demonstrates an initial prototype for a language learning application. The app predicts and translates words that a learner might find challenging based on their proficiency level and the words they mark as unknown.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Run All Cells**: To get started, you need to run all the cells in this notebook. This will install the necessary libraries, download required data, and set up the environment.\n",
        "\n",
        "2. **Interface Deployment**: Once all cells are run, a Gradio interface will be deployed at the bottom of this notebook. You will interact with this interface to use the app.\n",
        "\n",
        "3. **Using the Interface**:\n",
        "   - **Native Language**: Select your native language (currently only English is available).\n",
        "   - **Target Language**: Select the target language you want to learn (currently only Spanish is available).\n",
        "   - **Proficiency Level**: Choose your proficiency level from A1 to C2.\n",
        "   - **Text Input**: Enter the text you want to read and learn from in the target language.\n",
        "   - **Start**: Click the 'Start' button to begin the process. The app will process the text, predict unknown words, and provide translations.\n",
        "   - **Input Unknown Words**: You can input any additional unknown words you encounter.\n",
        "   - **Next Paragraph**: Click the 'Next Paragraph' button to process the next paragraph of text.\n",
        "   - **Restart**: If you want to start over, click the 'Restart' button to reset the interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik7j9REClybR"
      },
      "source": [
        "## Library Installation\n",
        "\n",
        "In this section, we install all the necessary libraries required for our language learning application. These libraries include NLP tools, translation services, frequency analysis tools, and the Gradio library for building the user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o_qyrBCKgKR"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q stanza deep-translator langdetect wordfreq wiktionaryparser nltk gradio rapidfuzz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFiX7B2XmMId"
      },
      "source": [
        "## Data Download\n",
        "\n",
        "We download and extract the CogNet data, which contains cognate pairs between English and Spanish. This data helps identify cognates in the text and provides accurate translations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NEN-SF3DvqM"
      },
      "outputs": [],
      "source": [
        "# Download and extract the CogNet data\n",
        "!wget https://github.com/kbatsuren/CogNet/raw/master/CogNet-v2.0.zip\n",
        "!unzip CogNet-v2.0.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnrn15Q9mchs"
      },
      "source": [
        "## Library Imports\n",
        "\n",
        "This section imports all the necessary libraries that we will use throughout the notebook. These libraries provide functionalities such as natural language processing, translation, and data manipulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFmC0P7rmd3j"
      },
      "outputs": [],
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "import io\n",
        "import gradio as gr\n",
        "import stanza\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordfreq import word_frequency\n",
        "from rapidfuzz import fuzz, process\n",
        "import pandas as pd\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwoBHTPAmn1Y"
      },
      "source": [
        "## NLP Tools Initialization\n",
        "\n",
        "We download the necessary language models for Stanza and initialize the pipelines for English and Spanish. These pipelines will handle tokenization (splitting text into words), lemmatization (reducing words to their base forms), part-of-speech tagging (identifying grammatical roles), and named entity recognition (detecting proper names and entities). Additionally, we initialize the Snowball Stemmer for Spanish, which will reduce words to their root forms, aiding in identifying morphological similarities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caxToWV9msLy"
      },
      "outputs": [],
      "source": [
        "# Download Stanza language models for English and Spanish\n",
        "stanza.download('en')\n",
        "stanza.download('es')\n",
        "\n",
        "# Initialize Stanza pipelines with specific components\n",
        "nlp_native = stanza.Pipeline('en', processors='tokenize,lemma,pos')\n",
        "nlp_target = stanza.Pipeline('es', processors='tokenize,lemma,pos,ner')\n",
        "\n",
        "# Initialize the Snowball Stemmer for Spanish\n",
        "stemmer = SnowballStemmer(\"spanish\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7A-PHUgnCzf"
      },
      "source": [
        "## Data Loading and Filtering\n",
        "\n",
        "We load the CogNet data into a DataFrame and filter it to get the Spanish-English cognates. This filtered data will help us identify cognates in the input text, which are words that have a common etymological origin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHRi4gvdBTEP"
      },
      "outputs": [],
      "source": [
        "# Load the CogNet TSV file into a DataFrame\n",
        "cognet_df = pd.read_csv(\n",
        "    'CogNet-v2.0.tsv', sep='\\t', header=None,\n",
        "    names=['concept_id', 'lang1', 'word1', 'lang2', 'word2', 'translit1', 'translit2'],\n",
        "    on_bad_lines='skip', engine='python'\n",
        ")\n",
        "\n",
        "# Filter the DataFrame to get Spanish-English cognates\n",
        "cognet_sp_en = cognet_df[\n",
        "    ((cognet_df['lang1'] == 'spa') & (cognet_df['lang2'] == 'eng')) |\n",
        "    ((cognet_df['lang1'] == 'eng') & (cognet_df['lang2'] == 'spa'))\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx2Qpp8KnSMg"
      },
      "source": [
        "## Initializing Variables\n",
        "\n",
        "This section initializes various variables and data structures that will hold the state of the application, such as paragraphs, known words, unknown words, and translations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hEnKsDNnUEJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define frequency thresholds for different proficiency levels\n",
        "frequency_thresholds = {\n",
        "    'A1': 0.0005,\n",
        "    'A2': 0.00005,\n",
        "    'B1': 0.00001,\n",
        "    'B2': 0.000005,\n",
        "    'C1': 0.000001,\n",
        "    'C2': 0.0000005\n",
        "}\n",
        "\n",
        "# Cache for translations to avoid repeated translations\n",
        "translation_cache = {}\n",
        "\n",
        "# Function to initialize the state variables\n",
        "def initialize_variables():\n",
        "    global state\n",
        "    state = {\n",
        "        'paragraphs': [],\n",
        "        'current_paragraph_index': 0,\n",
        "        'known_words': [],\n",
        "        'unknown_words': [],\n",
        "        'validated_translations': [],\n",
        "        'all_final_unknown_words': [],\n",
        "        'all_cognate_pairs': {},\n",
        "        'final_unknown_words_dict': defaultdict(set),\n",
        "        'original_word_mapping': {},\n",
        "        'native_language': '',\n",
        "        'target_language': '',\n",
        "        'level': '',\n",
        "        'final_unknown_word_counts': defaultdict(int),\n",
        "        'nlp_cache': {},\n",
        "        'frequency_cache': {},\n",
        "        'ner_cache': {},\n",
        "        'merged_paragraphs': []\n",
        "    }\n",
        "\n",
        "initialize_variables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRLMZpnGne6c"
      },
      "source": [
        "## Cognate Identification Function\n",
        "\n",
        "This function identifies cognates between Spanish and English words using the CogNet data and independent similarity checks. Cognates are words in two languages that have a common etymological origin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hIH4pFBbPy3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to identify cognates\n",
        "def find_cognates(spanish_sentences, english_sentences, cognet_df, similarity_threshold=65):\n",
        "    cognates = []\n",
        "\n",
        "    for sp_sentence, en_sentence in zip(spanish_sentences, english_sentences):\n",
        "        spanish_words = [word.lower() for word in sp_sentence.split()]\n",
        "        english_words = [word.lower() for word in en_sentence.split()]\n",
        "\n",
        "        # Check for cognates in CogNet\n",
        "        for sp_word in spanish_words:\n",
        "            matches = cognet_df[(cognet_df['word1'].str.lower() == sp_word) | (cognet_df['word2'].str.lower() == sp_word)]\n",
        "            for _, row in matches.iterrows():\n",
        "                if row['lang1'] == 'spa' and row['lang2'] == 'eng':\n",
        "                    en_word = row['word2'].lower()\n",
        "                elif row['lang1'] == 'eng' and row['lang2'] == 'spa':\n",
        "                    en_word = row['word1'].lower()\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if en_word in english_words:\n",
        "                    similarity = fuzz.ratio(sp_word, en_word)\n",
        "                    if similarity >= similarity_threshold:\n",
        "                        cognates.append((sp_word, en_word))\n",
        "                        print(f\"CogNet Cognate Identified: {sp_word} (Spanish) <-> {en_word} (English)\")\n",
        "\n",
        "        # Identify lemma-based cognates with POS tag check\n",
        "        for sp_word in spanish_words:\n",
        "            if len(sp_word) <= 3:\n",
        "                continue\n",
        "            sp_features = state['nlp_cache'].get(sp_word, {})\n",
        "            for en_word in english_words:\n",
        "                if len(en_word) <= 3:\n",
        "                    continue\n",
        "                en_features = state['nlp_cache'].get(en_word, {})\n",
        "\n",
        "                \"\"\"# Debugging: Print words being compared and their features\n",
        "                print(f\"Comparing: {sp_word} (Spanish) with {en_word} (English)\")\n",
        "                print(f\"  Similarity: {fuzz.ratio(sp_word, en_word)}\")\n",
        "                print(f\"  Spanish POS: {sp_features.get('pos')}\")\n",
        "                print(f\"  English POS: {en_features.get('pos')}\")\n",
        "                print(f\"  Spanish Lemma: {sp_features.get('lemma')}\")\n",
        "                print(f\"  English Lemma: {en_features.get('lemma')}\")\"\"\"\n",
        "\n",
        "                similarity = fuzz.ratio(sp_word, en_word)\n",
        "                if similarity >= 80:\n",
        "                    \"\"\"print(f\"Similarity Cognate Identified: {sp_word} (Spanish) <-> {en_word} (English)\")\"\"\"\n",
        "                    cognates.append((sp_word, en_word))\n",
        "                if similarity >= 50 and sp_features.get('pos') == en_features.get('pos'):\n",
        "                    \"\"\"print(f\"Similarity Cognate Identified: {sp_word} (Spanish) <-> {en_word} (English)\")\"\"\"\n",
        "                    cognates.append((sp_word, en_word))\n",
        "                if sp_features.get('pos') == en_features.get('pos') and similarity >= similarity_threshold:\n",
        "                    cognates.append((sp_word, en_word))\n",
        "                    \"\"\"print(f\"POS Cognate Identified: {sp_word} (Spanish) <-> {en_word} (English)\")\"\"\"\n",
        "                    if similarity >= similarity_threshold:\n",
        "                        if sp_features and en_features and sp_features.get('lemma') == en_features.get('lemma'):\n",
        "                            cognates.append((sp_word, en_word))\n",
        "                            \"\"\"print(f\"Lemma Cognate Identified: {sp_word} (Spanish) <-> {en_word} (English)\")\"\"\"\n",
        "\n",
        "    return cognates\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPG8D2VDoxVV"
      },
      "source": [
        "## Batch Translation Function\n",
        "\n",
        "Here we define the `batch_translate` function, which translates a list of sentences in batch mode. This function uses the GoogleTranslator to translate sentences from the target language to the native language. It also caches translations to avoid redundant API calls, improving performance. It uses retry logic to handle errors. The function attempts to translate each sentence up to three times before returning a fallback message. This ensures that temporary issues with the translation service do not cause the app to fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re7OhArMo08h"
      },
      "outputs": [],
      "source": [
        "# Batch translation\n",
        "def batch_translate(sentences, src, dest, max_retries=3):\n",
        "    translations = []\n",
        "    for sentence in sentences:\n",
        "        if sentence in translation_cache:\n",
        "            translations.append(translation_cache[sentence])\n",
        "        else:\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    translation = GoogleTranslator(source=src, target=dest).translate(sentence)\n",
        "                    if translation:\n",
        "                        translations.append(translation)\n",
        "                        translation_cache[sentence] = translation\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error translating sentence '{sentence}': {e} (Attempt {attempt + 1} of {max_retries})\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    translations.append(\"Translation not available\")\n",
        "                    translation_cache[sentence] = \"Translation not available\"\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Yh_b2lpD-C"
      },
      "source": [
        "## Morphological Similarity Check Function\n",
        "\n",
        "This section introduces the `is_similar_morphology` function. The function checks if two words are morphologically similar based on their stems and lemmas. This is useful for identifying words that are related or share a common root, which can help in predicting unknown words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AXUX_-EpGpn"
      },
      "outputs": [],
      "source": [
        "# Function to check morphological similarity\n",
        "def is_similar_morphology(word1, word2, threshold):\n",
        "    stem1, stem2 = word1['stem'], word2['stem']\n",
        "    lemma1, lemma2 = word1['lemma'], word2['lemma']\n",
        "    freq2 = word2['frequency']\n",
        "\n",
        "    if stem1 == stem2:\n",
        "        return True\n",
        "    if (stem1 in stem2 or stem2 in stem1) and freq2 < threshold:\n",
        "        return True\n",
        "    if lemma1 == lemma2:\n",
        "        return True\n",
        "    if (lemma1 in lemma2 or lemma2 in lemma1) and freq2 < threshold:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUYLp3FIpMKf"
      },
      "source": [
        "## Text Preprocessing Function\n",
        "\n",
        "In this section, we define the `batch_preprocess_text` function. This function tokenizes, lemmatizes, and performs part-of-speech tagging on the input text using Stanza. It also calculates word frequencies and caches the results to improve performance. The preprocessing steps are essential for understanding the structure of the text and identifying which words might be challenging for learners.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlwvqmvXpn4Z"
      },
      "outputs": [],
      "source": [
        "# Preprocess text (tokenize, lemmatize, POS tagging)\n",
        "def batch_preprocess_text(paragraphs, nlp, target_language, use_cache=True):\n",
        "    batch_text = \"\\n\\n\".join(paragraphs)\n",
        "    if use_cache and batch_text in state['nlp_cache']:\n",
        "        return state['nlp_cache'][batch_text]\n",
        "\n",
        "    doc = nlp(batch_text)\n",
        "    sentences = [sentence.text for sentence in doc.sentences]\n",
        "    words = []\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if len(word.text) > 1:  # Ensure we are processing only whole words\n",
        "                if word.text in state['frequency_cache']:\n",
        "                    frequency = state['frequency_cache'][word.text]\n",
        "                else:\n",
        "                    frequency = word_frequency(word.text, target_language)\n",
        "                    state['frequency_cache'][word.text] = frequency\n",
        "                word_features = {\n",
        "                    'text': word.text,\n",
        "                    'lemma': word.lemma,\n",
        "                    'pos': word.upos,\n",
        "                    'frequency': frequency,\n",
        "                    'stem': stemmer.stem(word.text)\n",
        "                }\n",
        "                words.append(word_features)\n",
        "                state['nlp_cache'][word.text.lower()] = word_features\n",
        "\n",
        "\n",
        "    if use_cache:\n",
        "        state['nlp_cache'][batch_text] = (sentences, words)\n",
        "    return sentences, words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Z5uNDrppsp"
      },
      "source": [
        "## Named Entity Recognition (NER) Function\n",
        "\n",
        "This section defines the `perform_ner` function, which performs named entity recognition on the input text using the Stanza pipeline. Named entities (e.g., names of people, places, organizations) are often known words for language learners, and recognizing them helps in accurately predicting unknown words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaxnK5WYpzkD"
      },
      "outputs": [],
      "source": [
        "# Perform Named Entity Recognition (NER)\n",
        "def perform_ner(text, nlp):\n",
        "    if text in state['ner_cache']:\n",
        "        return state['ner_cache'][text]\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities = [entity.text.lower() for sentence in doc.sentences for entity in sentence.ents]\n",
        "    state['ner_cache'][text] = entities\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-pBN8vNp1EI"
      },
      "source": [
        "## Translation Validation Function\n",
        "\n",
        "This section defines the `validate_translation_in_context` function, which aims to ensure that the translations make sense within the given context. The function uses several checks:\n",
        "\n",
        "1. **Initial Check (Direct Match)**: It directly matches the translated word with words in the translated sentence.\n",
        "2. **Similarity Check**: It uses fuzzy string matching to find the most similar word in the translated sentence.\n",
        "3. **POS Tag Check**: It matches the part-of-speech (POS) tag of the translated word with words in the translated sentence.\n",
        "\n",
        "These checks help in providing accurate translations that fit well in the context of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvqM2RyXC950"
      },
      "outputs": [],
      "source": [
        "# Validate translation with context\n",
        "def validate_translation_in_context(translation, original_sentences, translated_sentences, spanish_pos):\n",
        "    for orig_sent, trans_sent in zip(original_sentences, translated_sentences):\n",
        "        # Retrieve or compute NLP results for the translated sentence\n",
        "        trans_doc = state['nlp_cache'].get(trans_sent, nlp_native(trans_sent))\n",
        "        state['nlp_cache'][trans_sent] = trans_doc\n",
        "        if isinstance(trans_doc, tuple):\n",
        "            trans_doc = trans_doc[1]\n",
        "\n",
        "        # Retrieve or compute NLP results for the original sentence\n",
        "        orig_doc = state['nlp_cache'].get(orig_sent, nlp_target(orig_sent))\n",
        "        state['nlp_cache'][orig_sent] = orig_doc\n",
        "        if isinstance(orig_doc, tuple):\n",
        "            orig_doc = orig_doc[1]\n",
        "\n",
        "        # Initial Check: Direct match\n",
        "        for word in trans_doc.sentences[0].words:\n",
        "            if word.text.lower() == translation.lower():\n",
        "                return word.text\n",
        "\n",
        "        # Similarity Check: Find the most similar word\n",
        "        words_in_trans_sent = [word.text for word in trans_doc.sentences[0].words]\n",
        "        most_similar = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=80)\n",
        "        similar_enough = process.extractOne(translation, words_in_trans_sent, scorer=fuzz.ratio, score_cutoff=70)\n",
        "\n",
        "        if most_similar:\n",
        "            similar_word = most_similar[0]\n",
        "            for word in trans_doc.sentences[0].words:\n",
        "                if word.text == similar_word:\n",
        "                    return f\"{translation}/{similar_word}\"  # Return both the translation and the most similar word\n",
        "\n",
        "        if similar_enough:\n",
        "            similar_enough_word = similar_enough[0]\n",
        "            for word in trans_doc.sentences[0].words:\n",
        "                if word.text == similar_enough_word and word.upos == spanish_pos:\n",
        "                    return f\"{translation}/{similar_enough_word}\"\n",
        "\n",
        "        # POS Tag Check: Find a word with the same POS tag as the Spanish word\n",
        "        pos_matches = [word.text for word in trans_doc.sentences[0].words if word.upos == spanish_pos]\n",
        "        if len(pos_matches) == 1:\n",
        "            return f\"{translation}/{pos_matches[0]}\"  # Return both the translation and the POS matching word if there's only one match\n",
        "        elif len(pos_matches) > 1:\n",
        "            return translation  # Stick to the individual translation if multiple POS matches are found\n",
        "\n",
        "    # If no word with the same POS tag is found\n",
        "    return translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KL_q0jlqPe1"
      },
      "source": [
        "## Profiling Decorator\n",
        "\n",
        "This section introduces a decorator function `profile_func` that profiles the execution time of functions. It helps in identifying performance bottlenecks by logging the time taken by various parts of the code, which is crucial for optimizing the app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N82pcLx3qWPT"
      },
      "outputs": [],
      "source": [
        "# Profile Decorator\n",
        "def profile_func(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        pr = cProfile.Profile()\n",
        "        pr.enable()\n",
        "        result = func(*args, **kwargs)\n",
        "        pr.disable()\n",
        "        s = io.StringIO()\n",
        "        sortby = 'cumulative'\n",
        "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
        "        ps.print_stats(10)\n",
        "        print(s.getvalue())\n",
        "        return result\n",
        "    return wrapper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHW9QbPIrKf1"
      },
      "source": [
        "## Paragraph Processing Function\n",
        "\n",
        "This section defines the `process_paragraph` function, which processes each paragraph of text to predict unknown words, translate sentences, and validate translations. It involves several steps:\n",
        "\n",
        "1. **Preprocessing Text**: Tokenizes, lemmatizes, and performs POS tagging.\n",
        "2. **Named Entity Recognition**: Identifies named entities in the text.\n",
        "3. **Translation**: Translates sentences from the target language to the native language.\n",
        "4. **Identifying Cognates**: Finds cognates between the target language and the native language.\n",
        "5. **Identifying Unknown Words**: Determines which words are unknown to the learner based on frequency thresholds and similarity checks.\n",
        "\n",
        "This function combines these steps to provide a comprehensive analysis of each paragraph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zsP8cdKIkpO"
      },
      "outputs": [],
      "source": [
        "# Function to generate a word report\n",
        "def generate_word_report(word_report):\n",
        "    print(\"\\nWord Report:\")\n",
        "    for report in word_report:\n",
        "        print(f\"Word: {report['text']}\")\n",
        "        print(f\"  Is known word: {'yes' if report['is_known'] else 'no'}\")\n",
        "        print(f\"  Is input unknown word: {'yes' if report['is_input_unknown'] else 'no'}\")\n",
        "        print(f\"  Is within frequency threshold: {report['frequency_status']}\")\n",
        "        print(f\"  Is entity: {'yes' if report['is_entity'] else 'no'}\")\n",
        "        print(f\"  Is cognate: {report['is_cognate']}\")\n",
        "        if not report['is_known']:\n",
        "            print(f\"  Is morphologically related: {report['is_morphologically_related']}\")\n",
        "        print()\n",
        "\n",
        "# Translate and process each paragraph\n",
        "import re\n",
        "\n",
        "def normalize_word(word):\n",
        "    return re.sub(r'\\W+', '', word).lower()\n",
        "\n",
        "@profile_func\n",
        "def process_paragraph(paragraphs, input_unknown_words, known_words, unknown_words, validated_translations):\n",
        "    if not paragraphs:\n",
        "        return [], [], [], [], {}\n",
        "\n",
        "    current_paragraph_unknown_words = defaultdict(set)\n",
        "    sentences, words = batch_preprocess_text(paragraphs, nlp_target, state['target_language'])\n",
        "    entities = perform_ner(\"\\n\\n\".join(paragraphs), nlp_target)\n",
        "    translated_sentences = batch_translate(sentences, state['target_language'], state['native_language'])\n",
        "    threshold = frequency_thresholds[state['level']]\n",
        "\n",
        "    # Extract Spanish and English words\n",
        "    spanish_words = [normalize_word(word['text']) for word in words]\n",
        "    english_words = batch_translate([normalize_word(word['text']) for word in words], state['target_language'], state['native_language'])\n",
        "    cognates = find_cognates(sentences, translated_sentences, cognet_sp_en)\n",
        "    cognate_pairs = {normalize_word(sp): normalize_word(en) for sp, en in cognates}\n",
        "\n",
        "    word_report = []  # Initialize the list to store report details for each word\n",
        "\n",
        "    for word in words:\n",
        "        word_text = normalize_word(word['text'])\n",
        "        state['original_word_mapping'][word_text] = word['text']\n",
        "\n",
        "        report_details = {\n",
        "            'text': word['text'],\n",
        "            'is_known': False,\n",
        "            'is_input_unknown': word_text in input_unknown_words,\n",
        "            'frequency_status': 'above' if word['frequency'] >= threshold else 'below',\n",
        "            'is_entity': word_text in entities,\n",
        "            'is_cognate': cognate_pairs.get(word_text, 'no'),\n",
        "            'is_morphologically_related': 'no'  # Placeholder, will be updated later if necessary\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "        if word_text in entities or word['pos'] == 'PUNCT':\n",
        "            if word_text not in input_unknown_words:\n",
        "                known_words.append(word)\n",
        "                report_details['is_known'] = True\n",
        "        elif word['frequency'] >= threshold or word_text in cognate_pairs:\n",
        "            known_words.append(word)\n",
        "            report_details['is_known'] = True\n",
        "        else:\n",
        "            state['final_unknown_words_dict'][word_text].add(word['lemma'])\n",
        "            current_paragraph_unknown_words[word_text].add(word['lemma'])\n",
        "\n",
        "        word_report.append(report_details)  # Add the report details to the list\n",
        "\n",
        "    input_unknown_word_details = []  # Initialization here\n",
        "    for unknown_word in input_unknown_words:\n",
        "        processed_words = batch_preprocess_text([unknown_word], nlp_target, state['target_language'], use_cache=False)[1]\n",
        "        if processed_words:\n",
        "            processed_word = processed_words[0]\n",
        "            word_text = normalize_word(processed_word['text'])\n",
        "            input_unknown_word_details.append(processed_word)\n",
        "            state['final_unknown_words_dict'][word_text].add(processed_word['lemma'])\n",
        "            if word_text in spanish_words:\n",
        "                current_paragraph_unknown_words[word_text].add(processed_word['lemma'])\n",
        "        occurrences = spanish_words.count(word_text)\n",
        "\n",
        "        if word_text in state['final_unknown_word_counts']:\n",
        "            state['final_unknown_word_counts'][word_text] += occurrences\n",
        "        else:\n",
        "            state['final_unknown_word_counts'][word_text] = occurrences\n",
        "\n",
        "    for word in words:\n",
        "        for unknown_word in input_unknown_word_details:\n",
        "            if is_similar_morphology(word, unknown_word, threshold):\n",
        "                state['final_unknown_words_dict'][word['text'].lower()].add(word['lemma'])\n",
        "                current_paragraph_unknown_words[word['text']].add(word['lemma'])\n",
        "                # Update the report if the word is morphologically related\n",
        "                for report in word_report:\n",
        "                    if report['text'].lower() == word['text'].lower():\n",
        "                        report['is_morphologically_related'] = unknown_word['text']\n",
        "                        break\n",
        "\n",
        "    for word_text, lemmas in state['final_unknown_words_dict'].items():\n",
        "        for word in words:\n",
        "            if is_similar_morphology({'text': word_text, 'lemma': next(iter(lemmas)), 'stem': stemmer.stem(word_text)}, word, threshold):\n",
        "                current_paragraph_unknown_words[word['text']].add(word['lemma'])\n",
        "                # Update the report if the word is morphologically related\n",
        "                for report in word_report:\n",
        "                    if report['text'].lower() == word['text'].lower():\n",
        "                        report['is_morphologically_related'] = word_text\n",
        "                        break\n",
        "\n",
        "    for word in words:\n",
        "        word_text = word['text'].lower()\n",
        "        if word_text in current_paragraph_unknown_words:\n",
        "            if word_text in state['final_unknown_word_counts']:\n",
        "                state['final_unknown_word_counts'][word_text] += 1\n",
        "            elif state['final_unknown_word_counts'][word_text] >= 8:\n",
        "                del state['final_unknown_words_dict'][word_text]\n",
        "            else:\n",
        "                state['final_unknown_word_counts'][word_text] = 1\n",
        "\n",
        "\n",
        "    final_unknown_words = []\n",
        "    for word_text, lemmas in current_paragraph_unknown_words.items():\n",
        "        # Exclude cognates from final unknown words\n",
        "        if word_text in input_unknown_words or word_text not in cognate_pairs:\n",
        "            final_unknown_words.append({\n",
        "                'text': word_text,\n",
        "                'lemma': next(iter(lemmas)),\n",
        "                'pos': next((word['pos'] for word in words if word['text'].lower() == word_text), 'UNKNOWN'),\n",
        "                'frequency': next((word['frequency'] for word in words if word['text'].lower() == word_text), 0.0),\n",
        "                'stem': stemmer.stem(word_text)\n",
        "            })\n",
        "\n",
        "    def map_words_to_sentences(sentences, words):\n",
        "        sentence_word_map = {}\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            for word in words:\n",
        "                if word['text'].lower() in sentence.lower():\n",
        "                    sentence_word_map[word['text'].lower()] = (sentence, i)\n",
        "        return sentence_word_map\n",
        "\n",
        "    sentence_word_map = map_words_to_sentences(sentences, final_unknown_words)\n",
        "    for word in final_unknown_words:\n",
        "        if word['text'].lower() not in sentence_word_map:\n",
        "            continue\n",
        "        if word['text'] in translation_cache:\n",
        "            translation = translation_cache[word['text']]\n",
        "        else:\n",
        "            translation = GoogleTranslator(source=state['target_language'], target=state['native_language']).translate(word['text'])\n",
        "            translation_cache[word['text']] = translation\n",
        "        validated_translation = validate_translation_in_context(\n",
        "            translation,\n",
        "            sentences,\n",
        "            translated_sentences,\n",
        "            word['pos']\n",
        "        )\n",
        "        word['translation'] = validated_translation\n",
        "        word['sentence'] = sentence_word_map[word['text'].lower()][0]\n",
        "        word['translated_sentence'] = translated_sentences[sentence_word_map[word['text'].lower()][1]]\n",
        "        validated_translations.append({\n",
        "            'original': word['text'],\n",
        "            'translation': validated_translation,\n",
        "            'translated_pos': word['pos']\n",
        "        })\n",
        "\n",
        "    # Generate the word report\n",
        "    generate_word_report(word_report)\n",
        "\n",
        "    return sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BuShd8wrXQ0"
      },
      "source": [
        "## Starting Processing Function\n",
        "\n",
        "This section defines the `start_processing` function, which initializes the app state and begins processing the input text. The function sets the native and target languages, the proficiency level, and splits the input text into paragraphs. It then processes the first paragraph to start the app. The profiling decorator is used to measure the performance of this function.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Variables**: Reset all state variables to ensure a fresh start.\n",
        "2. **Set Language and Level**: Set the user's native language, target language, and proficiency level.\n",
        "3. **Split Text into Paragraphs**: Divide the input text into separate paragraphs for step-by-step processing.\n",
        "4. **Process First Paragraph**: Call `process_next_paragraph` to start processing the first paragraph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vwvDWRjr2Vi"
      },
      "outputs": [],
      "source": [
        "@profile_func\n",
        "def start_processing(native_language, target_language, level, text):\n",
        "    # Initialize all state variables\n",
        "    initialize_variables()\n",
        "\n",
        "    # Set user language preferences and proficiency level\n",
        "    state['native_language'] = native_language\n",
        "    state['target_language'] = target_language\n",
        "    state['level'] = level\n",
        "\n",
        "    # Split the input text into paragraphs for processing\n",
        "    state['paragraphs'] = text.strip().split('\\n')\n",
        "    state['current_paragraph_index'] = 0\n",
        "\n",
        "    # Start processing the first paragraph\n",
        "    return process_next_paragraph([])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PCH9GdLr24C"
      },
      "source": [
        "## Processing Next Paragraph Function\n",
        "\n",
        "The `process_next_paragraph` function processes the next paragraph of the text, updating the app state as it goes. This function is called repeatedly to process each paragraph in the input text.\n",
        "\n",
        "Key steps:\n",
        "1. **Check Paragraph Index**: If there are more paragraphs to process, it proceeds with the next one.\n",
        "2. **Process Paragraph**: Calls the `process_paragraph` function to handle the current paragraph.\n",
        "3. **Update State**: Updates the state variables with the processed data.\n",
        "4. **Display Output**: Formats and returns the processed paragraph with highlights and translations.\n",
        "5. **Generate Summary**: If all paragraphs are processed, it generates a summary of all unknown words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6tL8Wn-sHbh"
      },
      "outputs": [],
      "source": [
        "def process_next_paragraph(input_unknown_words):\n",
        "    global state\n",
        "# Check if there are more paragraphs to process\n",
        "    while state['current_paragraph_index'] < len(state['paragraphs']):\n",
        "        paragraph = state['paragraphs'][state['current_paragraph_index']].strip()\n",
        "\n",
        "        if paragraph:\n",
        "            sentences, translated_sentences, final_unknown_words, validated_translations, cognate_pairs = process_paragraph(\n",
        "                [paragraph],\n",
        "                input_unknown_words,\n",
        "                state['known_words'],\n",
        "                state['unknown_words'],\n",
        "                state['validated_translations']\n",
        "            )\n",
        "\n",
        "            # Update state with processed data\n",
        "            state['all_final_unknown_words'].extend(final_unknown_words)\n",
        "            state['all_cognate_pairs'].update(cognate_pairs)\n",
        "            output = display_output(paragraph, final_unknown_words)\n",
        "            state['current_paragraph_index'] += 1\n",
        "            print(f\"Paragraph {state['current_paragraph_index']} processed with input unknown words: {input_unknown_words}\")  # Debugging: Print paragraph processing status\n",
        "            return output\n",
        "\n",
        "        # If the paragraph is empty, skip to the next one\n",
        "        state['current_paragraph_index'] += 1\n",
        "\n",
        "    # If all paragraphs are processed, generate a summary\n",
        "    summary = generate_summary()\n",
        "    return f\"All paragraphs processed<br>{summary}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEFHMNfAsSUK"
      },
      "source": [
        "## Highlighted Paragraph Function\n",
        "\n",
        "The `highlighted_paragraph` function highlights unknown words in a paragraph by wrapping them in HTML tags and adding translations.\n",
        "\n",
        "Key steps:\n",
        "1. **Preserve Case**: The function preserves the original casing of the words when replacing them with highlighted versions.\n",
        "2. **Highlight Words**: Each unknown word is wrapped in `<b>` tags and its translation is appended in parentheses.\n",
        "3. **Return Highlighted Paragraph**: Returns the formatted paragraph with highlighted unknown words and translations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uGK1q1oI4QN"
      },
      "outputs": [],
      "source": [
        "def highlighted_paragraph(paragraph, final_unknown_words, validated_translations):\n",
        "    def preserve_case_replace(match, replacement):\n",
        "        matched_text = match.group()\n",
        "        if matched_text.isupper():\n",
        "            return replacement.upper()\n",
        "        elif matched_text[0].isupper():\n",
        "            return replacement.capitalize()\n",
        "        else:\n",
        "            return replacement\n",
        "\n",
        "    highlighted_paragraph = paragraph\n",
        "    for word in final_unknown_words:\n",
        "        original_word = state['original_word_mapping'].get(word['text'], word['text'])\n",
        "        translation_info = next((item for item in validated_translations if item['original'] == word['text']), None)\n",
        "        if translation_info:\n",
        "            translation = translation_info['translation']\n",
        "            highlighted_paragraph = re.sub(\n",
        "                r'(?i)\\b{}\\b'.format(re.escape(original_word)),\n",
        "                lambda match: preserve_case_replace(match, f\"<b>{match.group()}</b>({translation})\"),\n",
        "                highlighted_paragraph, flags=re.IGNORECASE\n",
        "            )\n",
        "\n",
        "    return highlighted_paragraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTkBVYazscVI"
      },
      "source": [
        "## Display Output Function\n",
        "\n",
        "This function, `display_output`, formats the processed paragraph and unknown words for display. It highlights unknown words in the paragraph and provides contextual sentences to help learners understand the usage of these words.\n",
        "\n",
        "Key steps:\n",
        "1. **Highlight Paragraph**: Calls `highlighted_paragraph` to get the formatted paragraph with highlighted unknown words.\n",
        "2. **Generate Contextual Sentences**: Creates sentences showing each unknown word in its context.\n",
        "3. **Format Output**: Combines the highlighted paragraph and contextual sentences into HTML format for display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2eHVqE9sqBq"
      },
      "outputs": [],
      "source": [
        "# Modified display_output function\n",
        "def display_output(paragraph, final_unknown_words):\n",
        "    highlighted_para = highlighted_paragraph(paragraph, final_unknown_words, state['validated_translations'])\n",
        "    context_sentences = []\n",
        "    for word in final_unknown_words:\n",
        "        translation = word.get('translation', 'No translation available')\n",
        "        context_sentence = f\"<b>{word['text']}:</b> <b>{translation}</b>.<br>{word.get('translated_sentence', 'No sentence available')}<br>\"\n",
        "        context_sentences.append(context_sentence)\n",
        "\n",
        "    context_output = \"<br>\".join(context_sentences)\n",
        "    original_paragraphs = paragraph.split(' ')\n",
        "    highlighted_original_para = highlighted_paragraph(\" \".join(original_paragraphs), final_unknown_words, state['validated_translations'])\n",
        "\n",
        "    return f\"<p><b style='font-size: larger;'>Highlighted Text:</b></p><p>{highlighted_original_para}</p><hr><p><b style='font-size: larger;'>Predicted Unknown Words In Context:</b></p><p>{context_output}</p>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wct3lElSssgs"
      },
      "source": [
        "## Generate Summary Function\n",
        "\n",
        "The `generate_summary` function compiles a summary of all unknown words encountered during text processing. It provides the count of appearances for each unknown word and its translation, helping learners review new vocabulary.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Summary**: Starts with a heading for the summary.\n",
        "2. **Compile Translations**: Gathers translations for all unknown words from the validated translations.\n",
        "3. **Format Summary**: Creates a summary listing each unknown word, its appearance count, and translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRQ8-MQYs2uz"
      },
      "outputs": [],
      "source": [
        "def generate_summary():\n",
        "    summary = \"<p style='font-size: larger;'><b>Summary of Unknown Words:</b></p><br>\"\n",
        "\n",
        "    translations_dict = {word: next((item for item in state['validated_translations'] if item['original'] == word), {}).get('translation', 'No translation found')\n",
        "                         for word in state['final_unknown_word_counts'].keys()}\n",
        "\n",
        "    for word, count in state['final_unknown_word_counts'].items():\n",
        "        translation = translations_dict.get(word, 'No translation found')\n",
        "        summary += f\"<b>{word}:</b> {count} appearances, Translation: {translation}<br>\"\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45P_SgtOs8Ia"
      },
      "source": [
        "## Next Paragraph Function\n",
        "\n",
        "This function, `next_paragraph`, is used to process the next paragraph in the text based on additional unknown words input by the user. It calls `process_next_paragraph` with the new unknown words and returns the output for the next paragraph.\n",
        "\n",
        "Key steps:\n",
        "1. **Convert Input to List**: Converts the input string of unknown words into a list.\n",
        "2. **Process Next Paragraph**: Calls `process_next_paragraph` with the list of input unknown words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehUu4tiAs9gn"
      },
      "outputs": [],
      "source": [
        "def next_paragraph(input_unknown_words):\n",
        "    if isinstance(input_unknown_words, str):\n",
        "        input_unknown_words = input_unknown_words.split()\n",
        "    return process_next_paragraph(input_unknown_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tR5msQStFdY"
      },
      "source": [
        "## Reset Interface Function\n",
        "\n",
        "The `reset_interface` function resets the entire interface and state variables, allowing the user to start over with a new text. This function is useful when the user wants to restart the session.\n",
        "\n",
        "Key steps:\n",
        "1. **Initialize Variables**: Calls `initialize_variables` to reset all state variables.\n",
        "2. **Update Interface**: Resets the Gradio interface elements to their initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YclfNWMPtUm9"
      },
      "outputs": [],
      "source": [
        "def reset_interface():\n",
        "    initialize_variables()\n",
        "    return gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value=''), gr.update(value='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LoWO6MmtiCQ"
      },
      "source": [
        "## Gradio Interface\n",
        "\n",
        "This section defines the Gradio interface for the language learning app. The interface includes inputs for the native language, target language, proficiency level, and text. It also provides buttons to start processing, move to the next paragraph, and restart the app.\n",
        "\n",
        "Key components:\n",
        "1. **Dropdowns**: For selecting native language, target language, and proficiency level.\n",
        "2. **Textbox**: For entering the text to be processed.\n",
        "3. **Buttons**: For starting the processing, moving to the next paragraph, and restarting the app.\n",
        "4. **Output Area**: Displays the processed text with highlighted unknown words and translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GufNasUbOri"
      },
      "outputs": [],
      "source": [
        "# Gradio Interface\n",
        "iface = gr.Blocks()\n",
        "\n",
        "with iface:\n",
        "    native_language_input = gr.Dropdown(choices=['en'], label='Native Language', value='en')\n",
        "    target_language_input = gr.Dropdown(choices=['es'], label='Target Language')\n",
        "    level_input = gr.Dropdown(choices=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], label='Level')\n",
        "    text_input = gr.Textbox(label='Text', lines=10)\n",
        "    start_button = gr.Button('Start')\n",
        "    output_area = gr.HTML()\n",
        "    unknown_words_input = gr.Textbox(label='Input Unknown Words', lines=2)\n",
        "    next_button = gr.Button('Next Paragraph')\n",
        "    restart_button = gr.Button('Restart')\n",
        "\n",
        "    start_button.click(start_processing, [native_language_input, target_language_input, level_input, text_input], [output_area])\n",
        "    next_button.click(next_paragraph, [unknown_words_input], [output_area])\n",
        "    restart_button.click(reset_interface, [], [native_language_input, target_language_input, level_input, text_input, output_area, unknown_words_input])\n",
        "\n",
        "iface.launch(share=True, debug=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQcfvvjrMsL"
      },
      "source": [
        "## How the code works\n",
        "The code takes a native language (currently English), a target language (currently Spanish), a proficiency level in the target language (from A1 to C2), and a text. It then uses Stanza to preprocess the text in batches. The preprocessing function divides the text into sentences (tokenization), extracts the base form of the words (lemmatization), and assigns a Part Of Speech tag (grammatical categories such as nouns, verbs, adjectives, adverbs, pronouns, etc.) to each word in a sentence. It also calculates the word frequencies and caches all results to improve performance.\n",
        "\n",
        "In the perform_ner function, the code uses Stanza to identify entities (proper names) in the text, which are stored in a special list to prevent them from being categorized as unknown. In the find_cognates function, the code checks if a word from the text is in the CogNet database. If it is, it adds the word and its translation to a cognates list. If the word is not found in the database, the code checks if the Spanish word shares the same lemma (base form) as the English translation or if the two words have a similarity threshold above 65%. Word pairs that meet any of these conditions are added to the cognates list to prevent them from being categorized as unknown.\n",
        "\n",
        "The code also includes a function called is_similar_morphology, which checks if two words are similar in morphology by comparing their stems (the word root, the minimal part of a word) and lemmas. This function predicts further unknown words given an initial list of unknown words by assuming that morphologically related words will also be unknown for the user.\n",
        "\n",
        "The code translates the preprocessed sentences from the target language to the native language using the Google Translator API. The translated sentences is used to ensure more contextually accurate translations.\n",
        "\n",
        "Here's how everything comes together. The code identifies unknown words as follows: if a word's frequency is below the threshold for the selected proficiency level, it is listed as unknown. Any word the user inputs is listed as unknown regardless of frequency. Any word in the text that is morphologically similar to a word in the list of unknown words is also marked as unknown. Proper names and cognates are not marked as unknown.\n",
        "\n",
        "Once the code identifies an unknown word, it translates it individually using the Google Translator API. This individual translation (which we can call 'the original translation') is then passed to the validate_translation_in_context_function. To validate the translation in context, the code first tries to find the exact same word in the corresponding translated sentence. If found, the translation is passed to be displayed. If not, the code looks for the most similar word in the translated sentence, with a similarity threshold of at least 80%. If still not found, the code looks for a word with at least 70% similarity and checks if it has the same Part Of Speech (POS) tag as the original word. If both conditions are met, it displays the contextual translation. If no word meets these conditions, the code checks if the unknown word's POS tag is unique in the original Spanish sentence. If it is, it finds the corresponding word in the translated sentence and displays the contextual translation. If multiple words share the same POS tag, only the original translation is displayed.\n",
        "\n",
        "The code displays a paragraph of the text with the identified unknown words highlighted in bold and their validated translations in parentheses. It also displays a list of the unknown words, their translations, and the full translated sentence to ensure the user has all the necessary context. The code tracks how many times a word has been translated, and after eight times, the word is removed from the unknown words list.\n",
        "\n",
        "The code iterates through each paragraph until all have been displayed. Finally, it presents a summary of the unknown words, their translations, and the number of times they were translated.\n",
        "\n",
        "To manage efficiency, the code profiles (measures) the execution time of various functions. This profiling helps identify performance bottlenecks. The specific parts profiled are: text preprocessing (the time taken for tokenization, lemmatization, and POS tagging); entity recognition (the time taken to identify proper names using the perform_ner function);\n",
        "cognate finding (the time taken to search the CogNet database and check for morphological similarity); the translation of the sentences using the Google Translator API; and the context validation (time taken to validate translations in context).\n",
        "By logging the time taken by these functions, the code can be optimized for better performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCxbIrpxzJf"
      },
      "source": [
        "## Experimentation and Tuning\n",
        "\n",
        "These are the parts of the code can be easily modified to experiment with different results and potentially improve the performance or accuracy of the prototype:\n",
        "\n",
        "### Fuzz Threshold in Cognate Identification\n",
        "\n",
        "In the `find_cognates` function, the `similarity_threshold` can be adjusted to see if it helps in better identifying cognates. The `similarity_threshold` determines how similar two words need to be to be considered cognates. This threshold ranges from 0 to 100, where a higher value means that the words need to be more similar to be identified as cognates. Setting a high threshold, such as 80 or 90, makes it stricter, so only very similar words are identified as cognates. This reduces false positives but might miss some valid cognates. Conversely, a lower threshold, such as 50 or 60, makes it more lenient, allowing more words to be identified as cognates, which can include false positives.\n",
        "\n",
        "### Frequency Thresholds for Different Levels\n",
        "\n",
        "The `frequency_thresholds` dictionary defines thresholds for different proficiency levels, determining which words are considered known or unknown based on their frequency in the language. These thresholds can be adjusted to see how it affects the identification of unknown words. For instance, lowering the threshold for the A1 level means that more words will be considered unknown, which might be less overwhelming for beginners. On the other hand, increasing the threshold will consider more words as known, potentially making the reading experience more challenging.\n",
        "\n",
        "### Translation Service\n",
        "\n",
        "The `safe_translate` function uses GoogleTranslator to translate sentences. Using a different translation service like YandexTranslator or DeepL might offer different levels of accuracy and performance, but each service has its limitations, such as API call limits, costs, or different degrees of language support.\n",
        "\n",
        "### Similarity Scoring Method\n",
        "\n",
        "The `rapidfuzz` library is also used for similarity scoring in the `validate_translation_in_context` function. You can experiment with different similarity scorers like `fuzz.token_sort_ratio` or `fuzz.partial_ratio`, and adjust the scoring cutoff values (threshold values) to see if it improves translation validation. Higher cutoff values make the matching criteria stricter, which can reduce false positives but might miss valid matches. Lowering the cutoff values can include more matches but at the risk of increased false positives. Changing the scoring method and cutoff values helps find the optimal balance for accurate translation validation.\n",
        "\n",
        "### Batch Size for Preprocessing\n",
        "\n",
        "The `batch_preprocess_text` function processes text in batches. Adjusting the batch size, such as processing paragraphs in smaller or larger batches, can impact efficiency and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri4xPST10gKL"
      },
      "source": [
        "## Evaluating the prototype\n",
        "Here are some ideas on how to measure the accuracy and effectiveness of the predictions:\n",
        "### 1. Ground Truth Data\n",
        "The first and easiest approach, which can be done without having users to test the app, is to use ground truth data, where we get or create pre-annotated texts with known unknown words for different proficiency levels. We can then compare the app's predictions with these annotations. Data for expected known and unknown words according to the proficiency level can be gathered from CEFR lexical sets (https://cvc.cervantes.es/ensenanza/biblioteca_ele/plan_curricular/niveles/08_nociones_generales_inventario_a1-a2.htm ). Another good place to look for data is [CL Anthology](https://aclanthology.org/search/?q=spanish+unknown+words), a repository of research papers in computational linguistics that often include datasets.\n",
        "#### Precision and Recall:\n",
        "Calculate precision and recall for the predicted unknown words compared to the ground truth annotations. Precision measures the proportion of correctly identified unknown words out of all predicted unknown words, while recall measures the proportion of correctly identified unknown words out of all actual unknown words.\n",
        "#### F1 Score:\n",
        "Compute the F1 score, which is the harmonic mean of precision and recall, to provide a single metric for evaluation.\n",
        "### 2. User Feedback\n",
        "When user feedback becomes available, we can ask users to mark which predicted unknown words they actually found challenging and measure the percentage of user agreement with the app's predictions.\n",
        "### 3. Weighted Precision and Recall\n",
        "For a more comprehensive evaluation, we should note that not all errors should be treated with the same importance. The severity of an error in the context of this language learning app can vary based on the type of word and its role in the sentence or text.\n",
        "To reflect the different severities of errors, we can use weighted precision and recall. Here, different types of errors are assigned different weights based on their importance.\n",
        "#### Error Severity\n",
        "Here are some examples of how we can classify the severity of the errors based on their impact on understanding the text:\n",
        "*   **False Positives (Unnecessary Translations)**: Less severe. These are words that the app translates but the user didn't need help with. They might create noise in the interface but they will not affect understanding.\n",
        "*   **False Negatives (Missed Translations)**: More severe. These are words that the app didn't translate but the user needed help with. They can significantly affect understanding.\n",
        "*   **Key Content Words**: Very severe if missed. These include nouns, verbs, adjectives, and adverbs that are crucial for understanding the sentence.\n",
        "*   **Function Words**: Less severe if missed. These include conjunctions, prepositions, articles, etc., that add grammatical structure but less semantic meaning.\n",
        "\n",
        "#### Importance-Based Weighting\n",
        "To implement the above, we can assign weights to different types of words and errors. For example:\n",
        "*   **False Negatives (Key Content Words)**: Weight = 3\n",
        "*   **False Negatives (Function Words)**: Weight = 2\n",
        "*   **False Positives (Key Content Words)**: Weight = 1\n",
        "*   **False Positives (Function Words)**: Weight = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr6Neg45arth"
      },
      "source": [
        "### Expanding to different languages\n",
        "To adapt the code for different language pairs we need to consider the a series of modifications. This involves updating language-specific components such as NLP models, translation services, frequency data, and handling morphological differences.\n",
        "1. NLP Models\n",
        "The current code uses Stanza for tokenization, lemmatization, and part-of-speech tagging. Stanza supports many languages, but not all. If we need to support a language that Stanza does not, we may need to switch to another NLP library such as SpaCy, which also supports a wide range of languages.\n",
        "It is important to ensure the new NLP model provides similar functionality (tokenization, lemmatization, POS tagging, NER). And to consider that the quality of NLP tasks may vary across languages, impacting accuracy.\n",
        "2. Translation Services\n",
        "The code currently uses GoogleTranslator from the deep-translator library. Alternative APIs like DeepL, Yandex, or Microsoft Translator can be considered if a specific language pair is not supported or if a more accurate service is needed for certain languages.\n",
        "Each translation service has different strengths. DeepL is known for high-quality translations but supports fewer languages compared to Google. Costs and API limits also vary.\n",
        "3. Frequency Data\n",
        "The current code uses the wordfreq library for frequency analysis. While it supports many languages, coverage and accuracy might vary. For languages not well-supported by wordfreq, we need to consider alternative frequency data sources or even corpora specific to those languages. Tools like Lexique3 for French or SUBTLEX for various languages can be useful.\n",
        "Frequency data quality is crucial for accurate unknown word prediction, so it is important to ensure that the frequency data is reliable and representative of modern language use.\n",
        "4. Morphological Differences\n",
        "Languages with rich morphology (like Finnish or Turkish) require a more robust morphological analysis. Morfessor or a similar tool can be used for such languages.\n",
        "The current code uses Snowball Stemmer for Spanish. For other languages, stemmers or lemmatizers might vary (for example using the ISRI Stemmer for Arabic).\n",
        "5. Named Entity Recognition (NER)\n",
        "The current code uses Stanza for NER, which supports many languages but its performance could vary depending on the language. If NER is not well-supported for a target language, we need to consider alternatives like SpaCy, Polyglot, or even custom-trained models.\n",
        "6. Cognate Identification\n",
        "Finding Cognates in New Language Pairs\n",
        "Technically CogNet has data for several language pairs, however its quality varies from language to language. Wiktionary or BabelNet can be potential sources for multilingual cognate data.\n",
        "7. User Interface Adjustments\n",
        "We will need to update the Gradio interface to ensure that it supports the input and display of characters from various languages, including those with different scripts (like Arabic).\n",
        "\n",
        "In summary, not all tools support all languages equally. We might need to mix and match different libraries for comprehensive support.\n",
        "These are some possible options for libraries and tools for some of the most used languages as target:\n",
        "1. English\n",
        "NLP: SpaCy, Stanza, NLTK\n",
        "Translation: Google Translator, DeepL, Microsoft Translator\n",
        "Frequency Data: SUBTLEX-US, wordfreq\n",
        "Morphological Analysis: NLTK, SpaCy\n",
        "NER: SpaCy, Stanza\n",
        "2. Mandarin Chinese\n",
        "NLP: Jieba (for tokenization), Stanza, SpaCy (with Chinese models)\n",
        "Translation: Google Translator, Baidu Translate, Microsoft Translator\n",
        "Frequency Data: Chinese Linguistic Data Consortium (CLDC), wordfreq\n",
        "Morphological Analysis: Jieba, Stanza\n",
        "NER: Stanford NLP, SpaCy, Stanza\n",
        "3. French\n",
        "NLP: SpaCy, Stanza, Talismane\n",
        "Translation: Google Translator, DeepL, Microsoft Translator\n",
        "Frequency Data: Lexique3, wordfreq\n",
        "Morphological Analysis: Talismane, SpaCy\n",
        "NER: SpaCy, Stanza\n",
        "4. Arabic\n",
        "NLP: Farasa, MADAMIRA, Stanza\n",
        "Translation: Google Translator, Microsoft Translator\n",
        "Frequency Data: Aralex (Arabic Lexicon Project), wordfreq\n",
        "Morphological Analysis: Farasa, MADAMIRA\n",
        "NER: Farasa, SpaCy (with Arabic models), Stanza\n",
        "5. Russian\n",
        "NLP: SpaCy, Stanza, Natasha\n",
        "Translation: Google Translator, Yandex Translate, Microsoft Translator\n",
        "Frequency Data: ruTenTen (Russian Web Corpus), wordfreq\n",
        "Morphological Analysis: Pymorphy2, SpaCy\n",
        "NER: Natasha, SpaCy, Stanza\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaQN2sRz6Yb"
      },
      "source": [
        "## Useful Resources for Identifying Difficult Words for Language Learners\n",
        "\n",
        "While specific datasets for testing the app prototype are not directly available, the following resources can provide valuable insights into word difficulty and vocabulary acquisition for language learners.\n",
        "\n",
        "### 1. Selecting Reading Texts Suitable for Incidental Vocabulary Learning by Considering the Estimated Distribution of Acquired Vocabulary\n",
        "This paper discusses a method for selecting reading texts that support incidental vocabulary learning by considering the distribution of vocabulary already acquired by learners. This approach helps identify which words might be more challenging based on learners' existing vocabulary.\n",
        "- **Link:** [Selecting Reading Texts Suitable for Incidental Vocabulary Learning](https://educationaldatamining.org/EDM2022/proceedings/2022.EDM-posters.99/)\n",
        "\n",
        "### 2. More Than Frequency: Exploring Predictors of Word Difficulty for Second Language Learners\n",
        "This research examines various predictors of word difficulty beyond frequency, including word length, concreteness, and phonological complexity. These insights help predict which words learners are likely to find challenging by considering a range of linguistic factors.\n",
        "- **Link:** [Exploring Predictors of Word Difficulty](https://www.researchgate.net/publication/333144175_More_Than_Frequency_Exploring_Predictors_of_Word_Difficulty_for_Second_Language_Learners)\n",
        "\n",
        "### 3. Harvard Dataverse: Second Language Acquisition Data\n",
        "This dataset includes extensive data on second language acquisition, covering learner interactions and vocabulary tests. Analyzing this dataset provides empirical evidence on common word difficulties and learner patterns, which can inform the app’s word prediction and translation algorithms.\n",
        "- **Link:** [Second Language Acquisition Data](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8SWHNO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF8Ly196lsH7"
      },
      "source": [
        "## Scaling Up\n",
        "The most computationally intensive processes in the current code are the translation and linguistic analysis tasks. The translation process, handled by the deep-translator library, creates a computational overhead due to its reliance on external translation API. This leads to inefficiencies in translation. Similarly, linguistic analysis performed by the Stanza library requires substantial computational resources, particularly for tasks like part-of-speech tagging (identifying the grammatical roles of words). These tasks demand memory and processing power, making them computationally intensive, especially as the size of the text increases.\n",
        "\n",
        "To address these challenges and improve performance, migrating to the Google Cloud Translation API offers several advantages. The API provides higher quality translations, increased efficiency, and scalability. The basic service has a price of 20 USD per million characters, which translates approximately 667 pages of text. If we estimate that language learners would read around 50-100 pages per month, the cost per user ranges from 1 USD to 2 USD monthly.\n",
        "\n",
        "Additionally, optimizing the code to leverage transformer-based models, such as those available in Hugging Face's Transformers library, can enhance linguistic analysis. These models offer superior capabilities in natural language processing tasks but require substantial computational resources, particularly GPUs, to operate efficiently. While the adoption of transformer models requires some  investment, the current code can still be optimizaded to improve performance without the need for advanced services yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-wvH7MUzEVH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}